
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>10. Selección y regularización de modelos lineales &#8212; Técnicas de Machine Learning con Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'StepRegularization';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="11. Árboles de decisión" href="Arboles.html" />
    <link rel="prev" title="9. Modelos de Probabilidad" href="ProbModels.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="Técnicas de Machine Learning con Python - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="Técnicas de Machine Learning con Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Machine Learning con Python
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introducción a Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="IntroduccionPython.html">1. Python, una introducción</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pandas.html">2. Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="Visualizacion.html">3. Gráficos básicos: matplotlib &amp; seaborn</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Análisis Exploratorio de Datos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Descriptiva.html">4. Estadística Descriptiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="PruebasHipotesis.html">5. Pruebas de Hipótesis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Aprendizaje Supervizado</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="IntroML.html">6. Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="AprendizajeSupervisado.html">7. Aprendizaje Supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="Regresion.html">8. Regresión lineal</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProbModels.html">9. Modelos de Probabilidad</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Selección y regularización de modelos lineales</a></li>
<li class="toctree-l1"><a class="reference internal" href="Arboles.html">11. Árboles de decisión</a></li>
<li class="toctree-l1"><a class="reference internal" href="SVM.html">12. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="RedesNeuronales.html">13. Redes Neuronales</a></li>
<li class="toctree-l1"><a class="reference internal" href="Evaluacion.html">14. Evaluación de Modelos</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Aprendizaje no Supervisado</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ACP.html">15. Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="NoSupervizado.html">16. Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="ReglasAso.html">17. Minería de Reglas de asociación</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/vmoprojs/MLPython" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/vmoprojs/MLPython/issues/new?title=Issue%20on%20page%20%2FStepRegularization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/StepRegularization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Selección y regularización de modelos lineales</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-variables">10.1. Selección de variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-las-mejores-variables">10.1.1. Selección de las mejores variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-adelante">10.1.2. Selección <em>stepwise</em> hacia adelante</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-atras">10.1.3. Selección <em>stepwise</em> hacia atrás</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo">10.1.3.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizacion">10.2. Regularización</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion">10.2.1. Introducción</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">10.2.1.1. Ejemplo</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#un-contexto-mas-general">10.2.2. Un contexto más general</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge">10.2.3. Regresión <em>Ridge</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lasso">10.2.4. Regresión <em>Lasso</em></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">10.2.4.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="seleccion-y-regularizacion-de-modelos-lineales">
<h1><span class="section-number">10. </span>Selección y regularización de modelos lineales<a class="headerlink" href="#seleccion-y-regularizacion-de-modelos-lineales" title="Link to this heading">#</a></h1>
<p>A pesar de su simplicidad, los modelos lineales tienen ventajas de interpretabilidad y con frecuencia muestran buen rendimiento en predicción</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_0+\beta_1X_1+\cdots+\beta_pX_p+\epsilon.
\]</div>
<p>Esta sección aborda el cambiar el ajuste de mínimos cuadrados ordinarios (OLS) tradicional por métodos alternativos que se apalancan en OLS.</p>
<p><strong>¿Por que usar alternativas a OLS?</strong></p>
<ul class="simple">
<li><p>Predictibilidad: Si la relación entre las variables es aproximadamente lineal y si <span class="math notranslate nohighlight">\(n\gg p\)</span> (el número de observaciones es más grande que el número de variables), entonces OLS tiene poco sesgo y rinde bien en datos <em>test</em>. Sin embargo, si <span class="math notranslate nohighlight">\(n\)</span> no es tan grande respecto a <span class="math notranslate nohighlight">\(p\)</span>, puede haber mucha variabilidad en OLS, lo que resulta en sobreajuste y mal rendimiento en <em>test</em>.</p></li>
</ul>
<p>Algunos de los problemas que surgen cuando <span class="math notranslate nohighlight">\(n\approx p\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Sobreajuste (Overfitting)</strong></p></li>
</ol>
<p>El <strong>error cuadrático medio</strong> (MSE) en el conjunto de entrenamiento se define como:</p>
<div class="math notranslate nohighlight">
\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> es el valor observado,</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> es la predicción del modelo,</p></li>
<li><p><span class="math notranslate nohighlight">\( n \)</span> es el número de observaciones.</p></li>
</ul>
<p>Cuando el número de variables <span class="math notranslate nohighlight">\(p\)</span> se aproxima a <span class="math notranslate nohighlight">\(n\)</span>, el modelo puede ajustarse casi perfectamente, lo que minimiza el MSE en el conjunto de entrenamiento, pero aumenta el error en nuevos datos (error de generalización). Este es el problema del sobreajuste.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Multicolinealidad</strong></p></li>
</ol>
<p>El <strong>problema de multicolinealidad</strong> se detecta cuando las variables explicativas están altamente correlacionadas entre sí. Un indicador común es el <strong>Factor de Inflación de la Varianza (VIF)</strong> para cada variable <span class="math notranslate nohighlight">\(j\)</span>, que se define como:</p>
<div class="math notranslate nohighlight">
\[
VIF_j = \frac{1}{1 - R_j^2}
\]</div>
<p>donde <span class="math notranslate nohighlight">\( R_j^2 \)</span> es el coeficiente de determinación de la regresión de la variable <span class="math notranslate nohighlight">\( X_j \)</span> sobre todas las demás variables.</p>
<p>Cuando <span class="math notranslate nohighlight">\( VIF_j \)</span> es grande (mayor a 10, por ejemplo), indica una alta colinealidad. Esto causa inestabilidad en las estimaciones de los coeficientes <span class="math notranslate nohighlight">\( \hat{\beta}_j \)</span>, lo que puede hacer que el modelo sea sensible a pequeños cambios en los datos.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Modelo No Identificable</strong></p></li>
</ol>
<p>En regresión lineal, los coeficientes de los parámetros <span class="math notranslate nohighlight">\( \boldsymbol{\beta} = (\beta_1, \beta_2, \dots, \beta_p) \)</span> se obtienen resolviendo el sistema de ecuaciones lineales:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{y} \)</span> es el vector de observaciones (de dimensión <span class="math notranslate nohighlight">\( n \times 1 \)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{X} \)</span> es la matriz de diseño (de dimensión <span class="math notranslate nohighlight">\( n \times p \)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span> es el vector de coeficientes,</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{\epsilon} \)</span> es el término de error.</p></li>
</ul>
<p>Para resolver <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span>, necesitamos invertir la matriz <span class="math notranslate nohighlight">\( \mathbf{X}^T \mathbf{X} \)</span>. Si <span class="math notranslate nohighlight">\( p \geq n \)</span>, la matriz <span class="math notranslate nohighlight">\( \mathbf{X}^T \mathbf{X} \)</span> no es invertible, lo que significa que el sistema es <strong>no identificable</strong> y no existe una única solución para <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span>.</p>
<div class="math notranslate nohighlight">
\[
\text{Si } \det(\mathbf{X}^T \mathbf{X}) = 0, \text{ entonces no hay solución única.}
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Varianza Alta</strong></p></li>
</ol>
<p>Cuando hay más variables que observaciones, el <strong>error estándar</strong> de los coeficientes de regresión se incrementa. La varianza de los coeficientes <span class="math notranslate nohighlight">\( \hat{\beta}_j \)</span> está dada por:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(\hat{\beta}_j) = \sigma^2 \left( (\mathbf{X}^T \mathbf{X})^{-1} \right)_{jj}
\]</div>
<p>donde <span class="math notranslate nohighlight">\( \sigma^2 \)</span> es la varianza del error residual. Si <span class="math notranslate nohighlight">\( \mathbf{X}^T \mathbf{X} \)</span> es mal condicionada (debido a la colinealidad o alta dimensionalidad), los elementos de la matriz inversa <span class="math notranslate nohighlight">\( (\mathbf{X}^T \mathbf{X})^{-1} \)</span> pueden ser muy grandes, lo que resulta en una varianza elevada para <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>. Esto significa que los coeficientes son altamente sensibles a los cambios en los datos.</p>
<ul class="simple">
<li><p>Interpretabilidad: Incluir variables independientes sin asociación con la variable dependiente en un modelo de regresión resulta en incluir complejidades innecesarias en el modelo. Si son removidas (se fijan sus coeficientes en cero), se puede obtener un modelo que es más interpretable. En OLS hay muy poca probabilidad de que se tengan coeficientes iguales a cero. Al eliminar variables estamos haciendo selección de variables o <em>feature selection</em>: selección de variables, contracción o regularización y reducción de dimensionalidad.</p></li>
</ul>
<section id="seleccion-de-variables">
<h2><span class="section-number">10.1. </span>Selección de variables<a class="headerlink" href="#seleccion-de-variables" title="Link to this heading">#</a></h2>
<section id="seleccion-de-las-mejores-variables">
<h3><span class="section-number">10.1.1. </span>Selección de las mejores variables<a class="headerlink" href="#seleccion-de-las-mejores-variables" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Algoritmo: Selección de las mejores variables</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. Sea <span class="math notranslate nohighlight">\(\mathcal{M_0}\)</span> el modelo nulo (sin predictores). Este modelo solo predice la media muestral para cada observación.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. Para <span class="math notranslate nohighlight">\(k = 1,2,\ldots,p\)</span>: <br/></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code> </code> a. Ajusta todos <span class="math notranslate nohighlight">\({p \choose k}\)</span> modelos que contienen exactamente <span class="math notranslate nohighlight">\(k\)</span> predictores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code> </code> b. Selecciona el mejor de los <span class="math notranslate nohighlight">\({p \choose k}\)</span> modelos, nómbralo <span class="math notranslate nohighlight">\(\mathcal{M_k}\)</span>. El <em>mejor</em> modelo se define como aquel que tiene el mejor <span class="math notranslate nohighlight">\(RRS\)</span> (suma de residuos al cuadrado), o de manera equivalente el más alto <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3. Selecciona el mejor modelo de los <span class="math notranslate nohighlight">\(\mathcal{M_0},\ldots,\mathcal{M_p}\)</span> usando el error de predicción en un conjunto de validación, <span class="math notranslate nohighlight">\(C_p\)</span>, AIC, BIC o <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. O usa el método de validación cruzada.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Ejemplo</strong></p>
<p>Supón que tienes estas variables para predecir <code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">final</span></code> (nota que <span class="math notranslate nohighlight">\(p=5\)</span>):</p>
<p><code class="docutils literal notranslate"><span class="pre">horas_estudio</span></code>, <code class="docutils literal notranslate"><span class="pre">uso_internet</span></code>, <code class="docutils literal notranslate"><span class="pre">nivel_socio</span></code>, <code class="docutils literal notranslate"><span class="pre">edad</span></code>, <code class="docutils literal notranslate"><span class="pre">género</span></code></p>
<p>Nota que en este caso tendrías la siguiente cantidad de modelos a probar:</p>
<div class="math notranslate nohighlight">
\[C(5,1)+C(5,2)+C(5,3)+C(5,4)+C(5,5)=5+10+10+5+1=32\]</div>
<p>En cada <span class="math notranslate nohighlight">\(k\)</span>, tendrás una elección final de todas las posibles:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Modelo</p></th>
<th class="head"><p>k</p></th>
<th class="head"><p>C(5, k)</p></th>
<th class="head"><p>Ejemplo de variables</p></th>
<th class="head"><p>Resultado</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(M_0\)</span></p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>Ninguna (modelo nulo)</p></td>
<td><p>Promedio general</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(M_1\)</span></p></td>
<td><p>1</p></td>
<td><p>5</p></td>
<td><p>horas_estudio</p></td>
<td><p>Mejor entre 5 modelos con 1 variable</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(M_2\)</span></p></td>
<td><p>2</p></td>
<td><p>10</p></td>
<td><p>horas_estudio + uso_internet</p></td>
<td><p>Mejor entre 10 modelos con 2 variables</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(M_3\)</span></p></td>
<td><p>3</p></td>
<td><p>10</p></td>
<td><p>horas_estudio + uso_internet + edad</p></td>
<td><p>Mejor entre 10 modelos con 3 variables</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(M_4\)</span></p></td>
<td><p>4</p></td>
<td><p>5</p></td>
<td><p>horas_estudio + uso_internet + edad + género</p></td>
<td><p>Mejor entre 5 modelos con 4 variables</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(M_5\)</span></p></td>
<td><p>5</p></td>
<td><p>1</p></td>
<td><p>Todas (modelo completo)</p></td>
<td><p>Único modelo con todas las variables</p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p>Nota que este algoritmo no puede ser aplicado cuando <span class="math notranslate nohighlight">\(p\)</span> es grande.</p></li>
<li><p>También puede tener problemas de estadísticos cuando <span class="math notranslate nohighlight">\(p\)</span> es grande. Cuanto mayor sea el espacio de búsqueda, mayor será la posibilidad de encontrar modelos que <strong>se vean bien en los datos de entrenamiento</strong>, aunque podrían no tener <strong>ningún poder predictivo</strong> sobre datos futuros.</p></li>
<li><p>Por lo tanto, un espacio de búsqueda enorme puede dar lugar a un sobreajuste y a una alta varianza de las estimaciones de los coeficientes.</p></li>
<li><p>Por los motivos antes listado, los métodos <em>stepwise</em> o <em>paso a paso</em> son más atractivos.</p></li>
</ul>
<p><strong><span class="math notranslate nohighlight">\(C_p\)</span> de Mallow</strong></p>
<div class="math notranslate nohighlight">
\[
C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)
\]</div>
<p>donde <span class="math notranslate nohighlight">\(d\)</span> es el total de parámetros usados y <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> es la estimación de la varianza del error <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p><strong>Criterio AIC</strong></p>
<p>Se puede usar en modelos donde el ajuste se realiza con máxima verosimilitud:</p>
<div class="math notranslate nohighlight">
\[
AIC = -2\text{log}L+2d
\]</div>
<p>Donde <span class="math notranslate nohighlight">\(L\)</span> es el valor máximo de la función de máxima verosimilitud del modelo.</p>
<div class="math notranslate nohighlight">
\[
BIC =  \frac{1}{n}(RSS+\text{log}(n)d\hat{\sigma}^2)
\]</div>
<p>donde <span class="math notranslate nohighlight">\(n\)</span> es el número de observaciones.</p>
<p><strong><span class="math notranslate nohighlight">\(R^2\)</span> ajustado</strong></p>
<div class="math notranslate nohighlight">
\[
R^2_{adj} = 1-\frac{RSS/(n-d-1)}{TSS/(n-1)}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(TSS\)</span> es la suma total de cuadrados.</p>
</section>
<section id="seleccion-stepwise-hacia-adelante">
<h3><span class="section-number">10.1.2. </span>Selección <em>stepwise</em> hacia adelante<a class="headerlink" href="#seleccion-stepwise-hacia-adelante" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Algoritmo: Selección <em>stepwise</em> hacia adelante</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. Sea <span class="math notranslate nohighlight">\(\mathcal{M_0}\)</span> el modelo nulo (sin predictores). Este modelo solo predice la media muestral para cada observación.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. Para <span class="math notranslate nohighlight">\(k = 0,2,\ldots,p-1\)</span>: <br/></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code> </code> a. Considera todos los <span class="math notranslate nohighlight">\(p-k\)</span> modelos que aumentan predictores en <span class="math notranslate nohighlight">\(\mathcal{M_k}\)</span> con un predictor adicional.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code> </code> b. Elige el mejor de estos <span class="math notranslate nohighlight">\(p-k\)</span> modelos, y nómbralo <span class="math notranslate nohighlight">\(\mathcal{M_{k+1}}\)</span>. El <em>mejor</em> modelo se define como aquel que tiene el mejor <span class="math notranslate nohighlight">\(RRS\)</span>, o el más alto <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3. Selecciona el mejor modelo de los <span class="math notranslate nohighlight">\(\mathcal{M_0},\ldots,\mathcal{M_p}\)</span> usando el error de predicción en un conjunto de validación, <span class="math notranslate nohighlight">\(C_p\)</span>, AIC, BIC o <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. O usa el método de validación cruzada.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Ejemplo</strong></p>
<p>Supón que tienes estas variables para predecir <code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">final</span></code> (nota que <span class="math notranslate nohighlight">\(p=5\)</span>):</p>
<p><code class="docutils literal notranslate"><span class="pre">horas_estudio</span></code>, <code class="docutils literal notranslate"><span class="pre">uso_internet</span></code>, <code class="docutils literal notranslate"><span class="pre">nivel_socio</span></code>, <code class="docutils literal notranslate"><span class="pre">edad</span></code>, <code class="docutils literal notranslate"><span class="pre">género</span></code></p>
<ol class="arabic simple">
<li><p>Empiezas sin ninguna <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(M_0\)</span></p></li>
<li><p>Pruebas una por una <span class="math notranslate nohighlight">\(\rightarrow\)</span> eliges la mejor <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(M_1\)</span> (<code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">~</span> <span class="pre">horas_estudio</span></code>)</p></li>
<li><p>Agregas otra de las restantes <span class="math notranslate nohighlight">\(\rightarrow\)</span> eliges la mejor <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(M_2\)</span> (<code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">~</span> <span class="pre">horas_estudio</span> <span class="pre">+</span> <span class="pre">uso_internet</span></code>)</p></li>
<li><p>Sigues así hasta probar todas</p></li>
<li><p>Eliges el mejor modelo entre todos (<span class="math notranslate nohighlight">\(M_0\)</span> a <span class="math notranslate nohighlight">\(M_5\)</span>), usando validación cruzada o AIC/BIC</p></li>
</ol>
</section>
<section id="seleccion-stepwise-hacia-atras">
<h3><span class="section-number">10.1.3. </span>Selección <em>stepwise</em> hacia atrás<a class="headerlink" href="#seleccion-stepwise-hacia-atras" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Algoritmo: Selección <em>stepwise</em> hacia atrás</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. Sea <span class="math notranslate nohighlight">\(\mathcal{M_p}\)</span> el modelo completo, que contiene todos los <span class="math notranslate nohighlight">\(p\)</span> predictores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. Para <span class="math notranslate nohighlight">\(k = p,p-1,\ldots,1\)</span>: <br/></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code> </code> a. Considera todos los <span class="math notranslate nohighlight">\(k\)</span> modelos que contienen todos menos uno de los predictores en en <span class="math notranslate nohighlight">\(\mathcal{M_k}\)</span>, para un total de <span class="math notranslate nohighlight">\(k-1\)</span> predictores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code> </code> b. Elige el mejor de estos <span class="math notranslate nohighlight">\(k\)</span> modelos, y nómbralo <span class="math notranslate nohighlight">\(\mathcal{M_{k-1}}\)</span>. El <em>mejor</em> modelo se define como aquel que tiene el mejor <span class="math notranslate nohighlight">\(RRS\)</span>, o el más alto <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3. Selecciona el mejor modelo de los <span class="math notranslate nohighlight">\(\mathcal{M_0},\ldots,\mathcal{M_p}\)</span> usando el error de predicción en un conjunto de validación, <span class="math notranslate nohighlight">\(C_p\)</span>, AIC, BIC o <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. O usa el método de validación cruzada.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Ejemplo</strong></p>
<p>Supón que tienes estas variables para predecir <code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">final</span></code> (nota que <span class="math notranslate nohighlight">\(p=5\)</span>):</p>
<p><code class="docutils literal notranslate"><span class="pre">horas_estudio</span></code>, <code class="docutils literal notranslate"><span class="pre">uso_internet</span></code>, <code class="docutils literal notranslate"><span class="pre">nivel_socio</span></code>, <code class="docutils literal notranslate"><span class="pre">edad</span></code>, <code class="docutils literal notranslate"><span class="pre">género</span></code></p>
<ol class="arabic simple">
<li><p>Empiezas con todas <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(M_5\)</span></p></li>
<li><p>Pruebas quitando una por una <span class="math notranslate nohighlight">\(\rightarrow\)</span> eliges la mejor <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(M_4\)</span> (<code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">~</span> <span class="pre">horas_estudio</span> <span class="pre">+</span> <span class="pre">uso_internet</span> <span class="pre">+</span> <span class="pre">nivel_socio</span> <span class="pre">+</span> <span class="pre">edad</span></code>). Aquí, por ejemplo, quitar <code class="docutils literal notranslate"><span class="pre">género</span></code> resultó ser la mejor opción.</p></li>
<li><p>Manteniendo <span class="math notranslate nohighlight">\(M_4\)</span>, quitas otra de las restantes <span class="math notranslate nohighlight">\(\rightarrow\)</span> eliges la mejor <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(M_3\)</span> (<code class="docutils literal notranslate"><span class="pre">nota</span> <span class="pre">~</span>&#160;&#160; <span class="pre">uso_internet</span> <span class="pre">+</span> <span class="pre">nivel_socio</span> <span class="pre">+</span> <span class="pre">edad</span></code>). Aquí, por ejemplo, quitar <code class="docutils literal notranslate"><span class="pre">horas_estudio</span></code> resultó ser la mejor opción.</p></li>
<li><p>Sigues así hasta probar quitnato todas</p></li>
<li><p>Eliges el mejor modelo entre todos (<span class="math notranslate nohighlight">\(M_0\)</span> a <span class="math notranslate nohighlight">\(M_5\)</span>), usando validación cruzada o AIC/BIC</p></li>
</ol>
<section id="ejemplo">
<h4><span class="section-number">10.1.3.1. </span>Ejemplo<a class="headerlink" href="#ejemplo" title="Link to this heading">#</a></h4>
<p>La superintendencia de compañias del Ecuador tiene información de estados financieros de las empresas que regula. Vamos a predecir los ingresos por ventas de las empresas grandes del 2023. En la pestaña <em>Recursos</em> del siguiente enlace: https://appscvsmovil.supercias.gob.ec/ranking/reporte.html</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">uu</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/vmoprojs/DataLectures/refs/heads/master/superciasGrandes2023.csv&quot;</span>
<span class="n">datos</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">uu</span><span class="p">)</span>
<span class="n">datos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">],</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Supongamos que &#39;X&#39; es tu conjunto de características y &#39;y&#39; la variable dependiente.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="o">!=</span><span class="s1">&#39;ingresos_ventas&#39;</span><span class="p">]</span>  <span class="c1"># tus datos de características</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">!=</span><span class="s1">&#39;costos_ventas_prod&#39;</span><span class="p">]</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">datos</span><span class="p">[</span><span class="s1">&#39;ingresos_ventas&#39;</span><span class="p">]</span><span class="o">/</span><span class="mi">1000000</span>  <span class="c1"># tu variable dependiente</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#import sweetviz as sv</span>

<span class="c1">#my_report = sv.analyze(datos)</span>
<span class="c1">#my_report.show_html()# altas correlaciones</span>
</pre></div>
</div>
</div>
</div>
<p>Notamos que existen variables con alta correlación, se eliminan correlaciones superiores a <span class="math notranslate nohighlight">\(0.9\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importa la librería NumPy, necesaria para operaciones numéricas con matrices y arrays</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Calcula la matriz de correlación de Pearson entre las variables de X (entre -1 y 1),</span>
<span class="c1"># y toma el valor absoluto para enfocarse solo en la fuerza de la relación (no en su dirección)</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>

<span class="c1"># Crea una matriz triangular superior (sin la diagonal) para evitar duplicados y la diagonal en la correlación</span>
<span class="c1"># np.triu(...) genera una matriz de unos en la parte superior, luego se convierte en booleano para usarla como máscara</span>
<span class="n">upper_triangle</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">))</span>

<span class="c1"># Recorre las columnas y selecciona aquellas que tienen al menos una correlación alta (&gt; 0.8) con otra variable</span>
<span class="n">high_corr_columns</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">upper_triangle</span><span class="o">.</span><span class="n">columns</span> 
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">upper_triangle</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Elimina del conjunto de datos X las columnas con alta correlación detectadas en el paso anterior</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">high_corr_columns</span><span class="p">)</span>

<span class="c1"># Imprime en consola la lista de nombres de las columnas que fueron eliminadas por estar altamente correlacionadas</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Columnas eliminadas por alta correlación:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">high_corr_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Columnas eliminadas por alta correlación:
 [&#39;patrimonio&#39;, &#39;impuesto_renta&#39;, &#39;utilidad_ejercicio&#39;, &#39;utilidad_neta&#39;, &#39;prueba_acida&#39;, &#39;end_largo_plazo&#39;, &#39;apalancamiento&#39;, &#39;apalancamiento_c_l_plazo&#39;, &#39;rot_activo_fijo&#39;, &#39;rent_ope_activo&#39;, &#39;roa&#39;, &#39;total_gastos&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">start_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:        ingresos_ventas   R-squared (uncentered):                   0.818
Model:                            OLS   Adj. R-squared (uncentered):              0.816
Method:                 Least Squares   F-statistic:                              425.0
Date:                Mon, 02 Jun 2025   Prob (F-statistic):                        0.00
Time:                        06:04:03   Log-Likelihood:                         -15624.
No. Observations:                2969   AIC:                                  3.131e+04
Df Residuals:                    2938   BIC:                                  3.150e+04
Df Model:                          31                                                  
Covariance Type:            nonrobust                                                  
=============================================================================================
                                coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------
activos                    4.267e-07   1.98e-08     21.527      0.000    3.88e-07    4.66e-07
utilidad_an_imp             2.59e-07   8.59e-08      3.017      0.003    9.07e-08    4.27e-07
n_empleados                   0.0252      0.002     13.497      0.000       0.022       0.029
liquidez_corriente            0.0171      0.014      1.227      0.220      -0.010       0.044
end_activo                   -0.1281      1.138     -0.113      0.910      -2.359       2.103
end_patrimonial              -0.0379      0.089     -0.426      0.670      -0.212       0.136
end_activo_fijo           -1.679e-05   5.77e-05     -0.291      0.771      -0.000    9.64e-05
end_corto_plazo              21.5596      1.986     10.855      0.000      17.665      25.454
cobertura_interes          4.176e-06   7.97e-06      0.524      0.600   -1.14e-05    1.98e-05
apalancamiento_financiero    -0.0020      0.005     -0.403      0.687      -0.011       0.008
end_patrimonial_ct           -0.0012      0.150     -0.008      0.994      -0.295       0.292
end_patrimonial_nct           0.6346      0.577      1.100      0.271      -0.496       1.765
rot_cartera                1.821e-05   1.95e-05      0.932      0.352   -2.01e-05    5.65e-05
rot_ventas                    0.0416      0.039      1.078      0.281      -0.034       0.117
per_med_cobranza          -8.883e-07   5.97e-07     -1.488      0.137   -2.06e-06    2.83e-07
per_med_pago                 1.7e-08   1.25e-08      1.361      0.174   -7.49e-09    4.15e-08
impac_gasto_a_v              -1.5197      0.725     -2.096      0.036      -2.941      -0.098
impac_carga_finan          -107.0103     22.392     -4.779      0.000    -150.917     -63.104
rent_neta_activo             14.2754      5.624      2.539      0.011       3.249      25.302
margen_bruto                -27.5007      3.178     -8.654      0.000     -33.732     -21.270
margen_operacional         6.395e-08    1.5e-08      4.254      0.000    3.45e-08    9.34e-08
rent_neta_ventas            -32.6691      7.673     -4.258      0.000     -47.713     -17.625
rent_ope_patrimonio          -0.0029      0.012     -0.236      0.814      -0.027       0.021
roe                           0.0333      0.055      0.604      0.546      -0.075       0.141
fortaleza_patrimonial        -0.4488      0.758     -0.592      0.554      -1.934       1.037
gastos_financieros         1.278e-06    4.1e-07      3.116      0.002    4.74e-07    2.08e-06
gastos_admin_ventas        1.142e-06   7.42e-08     15.384      0.000    9.96e-07    1.29e-06
depreciaciones            -7.185e-07      5e-07     -1.437      0.151    -1.7e-06    2.62e-07
amortizaciones            -3.584e-06   6.33e-07     -5.665      0.000   -4.82e-06   -2.34e-06
deuda_total                2.197e-06   1.38e-07     15.935      0.000    1.93e-06    2.47e-06
deuda_total_c_plazo        4.409e-06   4.75e-07      9.275      0.000    3.48e-06    5.34e-06
==============================================================================
Omnibus:                     4187.902   Durbin-Watson:                   2.054
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2749251.147
Skew:                           7.826   Prob(JB):                         0.00
Kurtosis:                     151.252   Cond. No.                     3.07e+09
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 3.07e+09. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Evaluación del modelo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importación de funciones métricas desde scikit-learn para evaluar modelos de regresión</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="c1"># Definición de una función personalizada para evaluar un modelo de regresión</span>
<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="c1"># Cálculo del error absoluto medio: promedio de las diferencias absolutas entre observados y predichos</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="c1"># Cálculo del error cuadrático medio: promedio de los cuadrados de las diferencias</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="c1"># Cálculo de la raíz del error cuadrático medio: facilita la interpretación porque está en las mismas unidades que la variable objetivo</span>
    <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
    
    <span class="c1"># Cálculo del coeficiente de determinación R²: indica la proporción de varianza explicada por el modelo</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="c1"># Retorna las cuatro métricas de evaluación como una tupla</span>
    <span class="k">return</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span><span class="p">,</span> <span class="n">r2</span>

<span class="c1"># Predicción de los valores utilizando un modelo OLS previamente entrenado llamado &#39;start_model&#39;</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">start_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Evaluación del modelo OLS comparando las predicciones con los valores reales (y)</span>
<span class="c1"># Almacena cada métrica en una variable correspondiente</span>
<span class="n">mae_ols</span><span class="p">,</span> <span class="n">mse_ols</span><span class="p">,</span> <span class="n">rmse_ols</span><span class="p">,</span> <span class="n">r2_ols</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred_ols</span><span class="p">)</span>

<span class="c1"># Impresión de los resultados de evaluación del modelo OLS</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">OLS Métricas de evaluación:&quot;</span><span class="p">)</span>
<span class="c1"># Error absoluto medio con dos decimales</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Absolute Error (MAE): </span><span class="si">{</span><span class="n">mae_ols</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Error cuadrático medio con dos decimales</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse_ols</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Raíz del error cuadrático medio con dos decimales</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Root Mean Squared Error (RMSE): </span><span class="si">{</span><span class="n">rmse_ols</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Coeficiente R² con dos decimales</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R-squared: </span><span class="si">{</span><span class="n">r2_ols</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OLS Métricas de evaluación:
Mean Absolute Error (MAE): 16.09
Mean Squared Error (MSE): 2179.19
Root Mean Squared Error (RMSE): 46.68
R-squared: 0.80
</pre></div>
</div>
</div>
</div>
<p>Modelo hacia adelante:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importación del selector secuencial de características desde sklearn y el modelo de regresión lineal</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Crear el modelo base: una regresión lineal ordinaria (no regularizada)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Crear el selector de características secuencial:</span>
<span class="c1"># - Usa el modelo lineal como evaluador</span>
<span class="c1"># - &#39;forward&#39; indica que se agregan características una a una (selección hacia adelante)</span>
<span class="c1"># - n_features_to_select=&#39;auto&#39; permite que el selector determine automáticamente cuántas características son óptimas</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;forward&#39;</span><span class="p">)</span>

<span class="c1"># Ajusta el selector a los datos: evalúa distintas combinaciones de variables para encontrar el mejor subconjunto</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Extrae un array booleano que indica qué variables fueron seleccionadas (True = seleccionada)</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>

<span class="c1"># Filtra X para quedarse solo con las columnas seleccionadas</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>

<span class="c1"># Ajusta un modelo OLS (de statsmodels) con las variables seleccionadas</span>
<span class="c1"># Nota: &#39;sm&#39; debe estar previamente importado como: import statsmodels.api as sm</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_selected</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Muestra el resumen completo del modelo OLS: incluye coeficientes, R², p-valores, etc.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:        ingresos_ventas   R-squared (uncentered):                   0.811
Model:                            OLS   Adj. R-squared (uncentered):              0.810
Method:                 Least Squares   F-statistic:                              842.7
Date:                Mon, 02 Jun 2025   Prob (F-statistic):                        0.00
Time:                        06:04:04   Log-Likelihood:                         -15680.
No. Observations:                2969   AIC:                                  3.139e+04
Df Residuals:                    2954   BIC:                                  3.148e+04
Df Model:                          15                                                  
Covariance Type:            nonrobust                                                  
=========================================================================================
                            coef    std err          t      P&gt;|t|      [0.025      0.975]
-----------------------------------------------------------------------------------------
activos                4.878e-07    1.4e-08     34.946      0.000     4.6e-07    5.15e-07
n_empleados               0.0208      0.002     12.851      0.000       0.018       0.024
end_corto_plazo          22.2718      1.836     12.130      0.000      18.672      25.872
end_patrimonial_ct       -0.0346      0.086     -0.401      0.689      -0.204       0.135
rot_cartera             1.94e-05   1.98e-05      0.978      0.328   -1.95e-05    5.83e-05
per_med_pago           1.367e-08   1.26e-08      1.080      0.280   -1.11e-08    3.85e-08
impac_gasto_a_v          -1.5230      0.731     -2.082      0.037      -2.957      -0.089
impac_carga_finan       -98.7982     21.518     -4.591      0.000    -140.990     -56.606
rent_neta_activo         17.6911      5.258      3.364      0.001       7.381      28.001
margen_bruto            -25.7358      3.143     -8.189      0.000     -31.898     -19.573
rent_neta_ventas        -35.7664      7.223     -4.952      0.000     -49.928     -21.604
fortaleza_patrimonial    -0.8198      0.719     -1.140      0.254      -2.230       0.591
gastos_admin_ventas    9.447e-07   5.34e-08     17.702      0.000     8.4e-07    1.05e-06
deuda_total            2.124e-06   1.33e-07     16.029      0.000    1.86e-06    2.38e-06
deuda_total_c_plazo     4.96e-06   4.71e-07     10.530      0.000    4.04e-06    5.88e-06
==============================================================================
Omnibus:                     4174.333   Durbin-Watson:                   2.035
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2485003.183
Skew:                           7.817   Prob(JB):                         0.00
Kurtosis:                     143.866   Cond. No.                     2.81e+09
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 2.81e+09. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Realiza predicciones sobre el conjunto de entrenamiento utilizando el modelo final ajustado con las variables seleccionadas</span>
<span class="c1"># &#39;X_selected&#39; contiene solo las variables elegidas por el proceso de selección secuencial hacia adelante</span>
<span class="n">y_pred_adelante</span> <span class="o">=</span> <span class="n">final_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_selected</span><span class="p">)</span>

<span class="c1"># Evalúa el desempeño del modelo usando la función &#39;evaluate_model&#39; previamente definida</span>
<span class="c1"># Calcula: MAE (error absoluto medio), MSE (error cuadrático medio), RMSE (raíz del error cuadrático medio) y R² (coeficiente de determinación)</span>
<span class="n">mae_adelante</span><span class="p">,</span> <span class="n">mse_adelante</span><span class="p">,</span> <span class="n">rmse_adelante</span><span class="p">,</span> <span class="n">r2_adelante</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred_adelante</span><span class="p">)</span>

<span class="c1"># Imprime en consola las métricas de evaluación del modelo construido tras la selección hacia adelante</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(Adelante) Métricas de evaluación:&quot;</span><span class="p">)</span>
<span class="c1"># Error absoluto medio</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Absolute Error (MAE): </span><span class="si">{</span><span class="n">mae_adelante</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Error cuadrático medio</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse_adelante</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Raíz del error cuadrático medio</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Root Mean Squared Error (RMSE): </span><span class="si">{</span><span class="n">rmse_adelante</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Coeficiente R²</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R-squared: </span><span class="si">{</span><span class="n">r2_adelante</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Adelante) Métricas de evaluación:
Mean Absolute Error (MAE): 16.51
Mean Squared Error (MSE): 2263.94
Root Mean Squared Error (RMSE): 47.58
R-squared: 0.79
</pre></div>
</div>
</div>
</div>
<p>Modelo hacia atrás</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importación del selector secuencial de características desde scikit-learn y del modelo de regresión lineal</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Crear el modelo base: en este caso, una regresión lineal ordinaria</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Crear el selector de características secuencial con selección hacia atrás</span>
<span class="c1"># - El modelo &#39;model&#39; se usa para evaluar qué variables eliminar</span>
<span class="c1"># - &#39;direction=&quot;backward&quot;&#39; indica que el selector comienza con todas las variables y elimina una por una</span>
<span class="c1"># - &#39;n_features_to_select=&quot;auto&quot;&#39; permite que el selector determine automáticamente cuántas conservar (basado en validación cruzada)</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;backward&#39;</span><span class="p">)</span>

<span class="c1"># Ajusta el selector de características a los datos</span>
<span class="c1"># Internamente realiza múltiples ajustes del modelo eliminando variables una a una</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Obtiene un array booleano que indica cuáles variables fueron finalmente seleccionadas (True = seleccionada)</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>

<span class="c1"># Filtra el DataFrame X para mantener solo las variables seleccionadas</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>

<span class="c1"># Ajusta un modelo de regresión OLS usando statsmodels con las variables seleccionadas</span>
<span class="c1"># Esto permite obtener un resumen estadístico completo del modelo (coeficientes, p-valores, etc.)</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_selected</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Imprime el resumen estadístico del modelo final seleccionado</span>
<span class="nb">print</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:        ingresos_ventas   R-squared (uncentered):                   0.810
Model:                            OLS   Adj. R-squared (uncentered):              0.809
Method:                 Least Squares   F-statistic:                              787.0
Date:                Mon, 02 Jun 2025   Prob (F-statistic):                        0.00
Time:                        06:04:06   Log-Likelihood:                         -15685.
No. Observations:                2969   AIC:                                  3.140e+04
Df Residuals:                    2953   BIC:                                  3.150e+04
Df Model:                          16                                                  
Covariance Type:            nonrobust                                                  
=======================================================================================
                          coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------
activos              4.838e-07   1.39e-08     34.757      0.000    4.56e-07    5.11e-07
n_empleados             0.0209      0.002     12.916      0.000       0.018       0.024
end_patrimonial        -0.0832      0.088     -0.946      0.344      -0.256       0.089
end_corto_plazo        23.6055      1.800     13.114      0.000      20.076      27.135
end_patrimonial_ct      0.0500      0.142      0.353      0.724      -0.228       0.328
end_patrimonial_nct     0.6900      0.586      1.177      0.239      -0.460       1.840
rot_cartera          2.081e-05   1.99e-05      1.048      0.295   -1.81e-05    5.98e-05
per_med_pago         1.239e-08   1.27e-08      0.977      0.329   -1.25e-08    3.73e-08
impac_gasto_a_v        -1.5354      0.733     -2.093      0.036      -2.973      -0.097
impac_carga_finan    -102.0002     21.526     -4.738      0.000    -144.208     -59.793
margen_bruto          -25.6326      3.150     -8.138      0.000     -31.809     -19.457
rent_neta_ventas      -29.5289      6.929     -4.261      0.000     -43.116     -15.942
roe                     0.0675      0.042      1.608      0.108      -0.015       0.150
gastos_admin_ventas   9.57e-07   5.33e-08     17.964      0.000    8.53e-07    1.06e-06
deuda_total          2.121e-06   1.33e-07     15.981      0.000    1.86e-06    2.38e-06
deuda_total_c_plazo  4.953e-06   4.72e-07     10.498      0.000    4.03e-06    5.88e-06
==============================================================================
Omnibus:                     4163.035   Durbin-Watson:                   2.040
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2455536.160
Skew:                           7.777   Prob(JB):                         0.00
Kurtosis:                     143.027   Cond. No.                     2.81e+09
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 2.81e+09. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predicciones y evaluación para Atras</span>
<span class="n">y_pred_atras</span> <span class="o">=</span> <span class="n">final_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_selected</span><span class="p">)</span>
<span class="n">mae_atras</span><span class="p">,</span> <span class="n">mse_atras</span><span class="p">,</span> <span class="n">rmse_atras</span><span class="p">,</span> <span class="n">r2_atras</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred_atras</span><span class="p">)</span>

<span class="c1"># Mostrar evaluación </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(Atrás) Métricas de evaluación:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Absolute Error (MAE): </span><span class="si">{</span><span class="n">mae_atras</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse_atras</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Root Mean Squared Error (RMSE): </span><span class="si">{</span><span class="n">rmse_atras</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R-squared: </span><span class="si">{</span><span class="n">r2_atras</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Atrás) Métricas de evaluación:
Mean Absolute Error (MAE): 16.66
Mean Squared Error (MSE): 2270.38
Root Mean Squared Error (RMSE): 47.65
R-squared: 0.79
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="regularizacion">
<h2><span class="section-number">10.2. </span>Regularización<a class="headerlink" href="#regularizacion" title="Link to this heading">#</a></h2>
<section id="introduccion">
<h3><span class="section-number">10.2.1. </span>Introducción<a class="headerlink" href="#introduccion" title="Link to this heading">#</a></h3>
<p>Las <strong>curvas de nivel</strong> (o superficies en dimensiones mayores) de la función de pérdida del modelo, usualmente el <strong>error cuadrático residual (RSS)</strong> en regresión lineal:</p>
<div class="math notranslate nohighlight">
\[
\text{RSS}(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2
\]</div>
<p>Un <strong>contorno</strong> es el conjunto de puntos <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span> donde la función de pérdida tiene el mismo valor:</p>
<div class="math notranslate nohighlight">
\[
\{\boldsymbol{\beta} : \text{RSS}(\boldsymbol{\beta}) = c\}
\]</div>
<p><strong>La función de pérdida como elipses</strong></p>
<p>La función <span class="math notranslate nohighlight">\( \text{RSS}(\boldsymbol{\beta}) \)</span> es una <strong>función cuadrática convexa</strong> respecto a <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span>, y se puede reescribir matricialmente como:</p>
<div class="math notranslate nohighlight">
\[
\text{RSS}(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
\]</div>
<p>Esto da lugar a:</p>
<div class="math notranslate nohighlight">
\[
\text{RSS}(\boldsymbol{\beta}) = \boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} - 2\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta} + \mathbf{y}^\top \mathbf{y}
\]</div>
<p>Esta es una <strong>forma cuadrática en <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span></strong>, lo cual en geometría implica que sus curvas de nivel son <strong>elipses</strong> (o hiperesferas si los predictores están ortogonalizados y escalados).</p>
<p><strong>Intuición geométrica</strong></p>
<ul class="simple">
<li><p>Cada <strong>elipse</strong> representa un nivel constante de error de predicción.</p></li>
<li><p>El <strong>centro de las elipses</strong> es el estimador de mínimos cuadrados <span class="math notranslate nohighlight">\( \boldsymbol{\beta}_{\text{OLS}} \)</span>.</p></li>
<li><p>La <strong>forma y orientación</strong> de las elipses depende de la matriz <span class="math notranslate nohighlight">\( \mathbf{X}^\top \mathbf{X} \)</span>, es decir, de la correlación entre predictores.</p></li>
<li><p>En 2D (por ejemplo, <span class="math notranslate nohighlight">\( \beta_1 \)</span>, <span class="math notranslate nohighlight">\( \beta_2 \)</span>), si hay correlación, las elipses están <em>inclinadas</em>.</p></li>
</ul>
<p><strong>¿Por qué esto es útil para Ridge y Lasso?</strong></p>
<p>Al aplicar penalización:</p>
<ul class="simple">
<li><p>Se <strong>superpone una región de restricciones</strong> sobre los coeficientes:</p>
<ul>
<li><p>Un <strong>círculo</strong> en Ridge (penalización L2).</p></li>
<li><p>Un <strong>rombo</strong> en Lasso (penalización L1).</p></li>
</ul>
</li>
<li><p>El valor óptimo de <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span> es el punto <strong>donde el primer contorno toca esa región de penalización</strong> (intersección tangente).</p></li>
</ul>
<section id="id1">
<h4><span class="section-number">10.2.1.1. </span>Ejemplo<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>Supongamos que tenemos el siguiente modelo:</p>
<p><span class="math notranslate nohighlight">\(
y = \beta_1 x_1 + \beta_2 x_2
\)</span></p>
<p>Y que nuestra matriz de diseño <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> y el vector de respuesta <span class="math notranslate nohighlight">\( \mathbf{y} \)</span> son:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} =
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
1 &amp; 1 \\
\end{bmatrix}
\quad
\text{y}
\quad
\mathbf{y} =
\begin{bmatrix}
1 \\
1 \\
2 \\
\end{bmatrix}
\end{split}\]</div>
<p>La función de pérdida (RSS) es:</p>
<div class="math notranslate nohighlight">
\[
\text{RSS}(\beta_1, \beta_2) = \sum_{i=1}^3 (y_i - x_{i1}\beta_1 - x_{i2}\beta_2)^2
\]</div>
<p>Podemos escribirla explícitamente:</p>
<div class="math notranslate nohighlight">
\[
= (1 - \beta_1)^2 + (1 - \beta_2)^2 + (2 - \beta_1 - \beta_2)^2
\]</div>
<p>Expandiendo los términos:</p>
<div class="math notranslate nohighlight">
\[
= (\beta_1 - 1)^2 + (\beta_2 - 1)^2 + (\beta_1 + \beta_2 - 2)^2
\]</div>
<p>Agrupando términos, obtenemos una <strong>forma cuadrática</strong> en <span class="math notranslate nohighlight">\( \beta_1 \)</span> y <span class="math notranslate nohighlight">\( \beta_2 \)</span>. En geometría, esta clase de expresiones define una <strong>cónica</strong>, y si la forma es convexa (como aquí, donde todos los coeficientes cuadráticos son positivos), la cónica resultante es una <strong>elipse</strong>.</p>
<p>Nota que</p>
<div class="math notranslate nohighlight">
\[(\beta_1 - 1)^2 + (\beta_2 - 1)^2 + (\beta_1 + \beta_2 - 2)^2 = 2\beta_1^2 + 2\beta_2^2 + 2\beta_1\beta_2 - 6\beta_1 - 6\beta_2 + 6\]</div>
<p>que corresponde a la forma general de una <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections">cónica</a>:</p>
<div class="math notranslate nohighlight">
\[
Q(x, y) = Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Matriz X y vector y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Estimación OLS: (X&#39;X)^(-1) X&#39;y</span>
<span class="n">XtX</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
<span class="n">Xty</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">beta_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XtX</span><span class="p">)</span> <span class="o">@</span> <span class="n">Xty</span>
<span class="n">beta1_hat</span><span class="p">,</span> <span class="n">beta2_hat</span> <span class="o">=</span> <span class="n">beta_ols</span>

<span class="c1"># Crear grilla para beta1 y beta2</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">B1</span><span class="p">,</span> <span class="n">B2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">)</span>

<span class="c1"># Calcular el RSS para cada combinación</span>
<span class="n">RSS</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">B1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">B2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span> <span class="o">-</span> <span class="n">B1</span> <span class="o">-</span> <span class="n">B2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Graficar</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">B1</span><span class="p">,</span> <span class="n">B2</span><span class="p">,</span> <span class="n">RSS</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contours</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Punto OLS</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">beta1_hat</span><span class="p">,</span> <span class="n">beta2_hat</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;OLS mínimo&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">beta1_hat</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">beta2_hat</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;OLS (</span><span class="si">{</span><span class="n">beta1_hat</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">beta2_hat</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_1$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Curvas de nivel del RSS con estimación OLS&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ea06d79373dc9b74262dd004ad4c1e541d6b0988e00fe3ff4d6f6ad7934cc59b.png" src="_images/ea06d79373dc9b74262dd004ad4c1e541d6b0988e00fe3ff4d6f6ad7934cc59b.png" />
</div>
</div>
</section>
</section>
<section id="un-contexto-mas-general">
<h3><span class="section-number">10.2.2. </span>Un contexto más general<a class="headerlink" href="#un-contexto-mas-general" title="Link to this heading">#</a></h3>
<p>La regularización es un método que nos permite restringir el proceso de estimación, se usa para evitar un posible sobre ajuste del modelo (<em>overfitting</em>).</p>
<p>Una forma de hacer regularización es donde los coeficientes de variables en el modelo no sean muy grandes (regresión <em>ridge</em>). Otra forma es contraer los coeficientes, llegando a tener coeficientes iguales a cero (regresión <em>lasso</em>).</p>
<p>Como regla general, el segundo enfoque suele ser mejor que el primero.</p>
<p>Los coeficientes regularizados se obtiene usando una función de penalidad <span class="math notranslate nohighlight">\(p(\boldsymbol{\alpha})\)</span> para restringir el tamaño del vector de coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = (\alpha_1,\cdots,\alpha_M)^T\)</span> del predictor <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{j = 1}^{M} \alpha_jC_j(\mathbf{x})\)</span>. Los coeficientes penalizados son obtenidos como solución al problema de minimización:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\alpha}}_{\lambda}= \underset{\boldsymbol{\alpha}}{\mathrm{argmin}} = \left\{\sum_{i = 1}^{n}L(y_i,f(\mathbf{x}_i))+\lambda p(\boldsymbol{\alpha}) \right\},
\]</div>
<p>donde <span class="math notranslate nohighlight">\(L\)</span> es una función de pérdida y <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> es un parámetro de regularización también conocido como <em>ratio de aprendizaje</em>.</p>
<p>Existen diferentes opciones para funciones de pérdida:</p>
<ul class="simple">
<li><p>Exponencial:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f(\mathbf{x})) = e^{-yf(\mathbf{x})}, y \in \{-1,+1\}.
\]</div>
<ul class="simple">
<li><p>Logística:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f_t(\mathbf{x})) = \text{log}\{1+ e^{-2yf_t(\mathbf{x})}\},y \in \{-1,+1\}.
\]</div>
<ul class="simple">
<li><p>Error cuadrático:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f_t(\mathbf{x})) = \frac{1}{2}(y-f_t(\mathbf{x}))^2, y \in \mathcal{R}.
\]</div>
<ul class="simple">
<li><p>Error absoluto:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f_t(\mathbf{x})) =  |y-f_t(\mathbf{x})|, y \in \mathcal{R}.
\]</div>
<ul class="simple">
<li><p>Huber</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
L(y,f_t(\mathbf{x})) = \begin{cases}
\frac{1}{2}(y-f_t(\mathbf{x}))^2, &amp; \text{if } |y-f_t(\mathbf{x})|\leq\delta,\\
\delta(|y-f_t(\mathbf{x})|-\delta/2), &amp; \text{en otro caso.}
\end{cases}
\end{split}\]</div>
<a class="reference internal image-reference" href="_images/L1_fig1.png"><img alt="_images/L1_fig1.png" src="_images/L1_fig1.png" style="width: 500px; height: 300px;" />
</a>
<p>Hay dos tipos de funciones de penalidad:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_2\)</span>: esta función de penalidad restringe la suma de cuadrados de los coeficientes,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_2(\boldsymbol{\alpha})=\sum_{j = 1}^{M} \alpha_j^2.
\]</div>
<p>Cuando <span class="math notranslate nohighlight">\(L\)</span> combinado es una combinación convexa y usamos la pérdida de error cuadrático, el predictor de regresión penalizado óptimo es el estimador de regresión <em>ridge</em>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_1\)</span>: Los coeficientes se restringen tal que su suma de valores absolutos,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_1(\boldsymbol{\alpha})=\sum_{j = 1}^{M} |\alpha_j|.
\]</div>
<p>sea menor que un valor dado. La evidencia empírica sugiere que la penalización <span class="math notranslate nohighlight">\(L_1\)</span> (<em>lasso</em>) funciona mejor cuando hay un número pequeño o mediano de coeficientes verdaderos de tamaño moderado.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Crear una cuadrícula de valores para beta1 y beta2</span>
<span class="n">beta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">B1</span><span class="p">,</span> <span class="n">B2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">)</span>

<span class="c1"># Simular una función de pérdida cuadrática (RSS) con mínimos en (2, 1)</span>
<span class="n">RSS</span> <span class="o">=</span> <span class="p">(</span><span class="n">B1</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">B2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Crear la figura</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># -------- Ridge (L2) --------</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">B1</span><span class="p">,</span> <span class="n">B2</span><span class="p">,</span> <span class="n">RSS</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<span class="n">circle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Ridge: Penalización L2 (Círculo)&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_2$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># -------- Lasso (L1) --------</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">B1</span><span class="p">,</span> <span class="n">B2</span><span class="p">,</span> <span class="n">RSS</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">)</span>
<span class="c1"># Dibujar el rombo para la norma L1 (|β1| + |β2| ≤ 2)</span>
<span class="n">l1_boundary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">l1_boundary</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">l1_boundary</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Lasso: Penalización L1 (Rombo)&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta_2$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/65b31859b64619064206682533a41595eb7947b95f01999e2b0e9cac9a396dc5.png" src="_images/65b31859b64619064206682533a41595eb7947b95f01999e2b0e9cac9a396dc5.png" />
</div>
</div>
</section>
<section id="regresion-ridge">
<h3><span class="section-number">10.2.3. </span>Regresión <em>Ridge</em><a class="headerlink" href="#regresion-ridge" title="Link to this heading">#</a></h3>
<p>En mínimos cuadrados ordinarios, las estimaciones de <span class="math notranslate nohighlight">\(\beta_0,\beta_1,\ldots,\beta_p\)</span> se obtienen minimizando</p>
<div class="math notranslate nohighlight">
\[
\text{RSS} = \sum_{i = 1}^n\left(y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right)^2.
\]</div>
<p>La <strong>regresión <em>ridge</em></strong> es muy similar al enfoque de mínimos cuadrados, pero los coeficientes de la regresión <em>ridge</em> <span class="math notranslate nohighlight">\(\hat{\beta}^R\)</span> son los valores que minimizan</p>
<div class="math notranslate nohighlight">
\[
\sum_{i = 1}^n\left(y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right)^2+\lambda\sum_{j = 1}^{p} \beta_j^2 = \text{RSS}+\lambda\sum_{j = 1}^{p} \beta_j^2,
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\lambda\)</span> es un parámetro ajustable que se determina de manera separada. La influencia de la regularización se controla con <span class="math notranslate nohighlight">\(\lambda\)</span>. Valores altos de  <span class="math notranslate nohighlight">\(\lambda\)</span> significa más regularización y modelos más simples. Sin embargo, la regresión <code class="docutils literal notranslate"><span class="pre">ridge</span></code> siempre generará un modelo que incluya todos los predictores. Incrementar el valor de <span class="math notranslate nohighlight">\(\lambda\)</span> tenderá a reducir las magnitudes de los coeficientes, pero no resultará en la exclusión de ninguna de las variables. Notemos que la penalidad no se aplica a <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</section>
<section id="regresion-lasso">
<h3><span class="section-number">10.2.4. </span>Regresión <em>Lasso</em><a class="headerlink" href="#regresion-lasso" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\sum_{i = 1}^n\left(y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right)^2+\lambda\sum_{j = 1}^{p} |\beta_j| = \text{RSS}+\lambda\sum_{j = 1}^{p} |\beta_j|,
\]</div>
<p>La penalización <span class="math notranslate nohighlight">\(L_1\)</span> tiene el efecto de forzar algunas de las estimaciones de coeficientes a ser <strong>exactamente iguales a cero</strong> cuando el parámetro <span class="math notranslate nohighlight">\(\lambda\)</span> de ajuste es suficientemente grande. Por lo tanto, <em>lasso</em> realiza <strong>selección de variables</strong>. Como resultado, los modelos generados a partir de <em>lasso</em> son generalmente mucho más <strong>fáciles de interpretar</strong> que los producidos por regresión <em>ridge</em>. Decimos que <em>lasso</em> produce modelos dispersos (<em>sparse</em>), es decir, modelos que involucran solo un subconjunto de variables predictoras.</p>
<section id="id2">
<h4><span class="section-number">10.2.4.1. </span>Ejemplo<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importación de bibliotecas necesarias</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># DIVISIÓN DE LOS DATOS EN ENTRENAMIENTO Y PRUEBA</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Divide el conjunto de datos en entrenamiento (80%) y prueba (20%)</span>
<span class="c1"># El parámetro random_state garantiza reproducibilidad</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">85</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># ESCALAMIENTO DE VARIABLES</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Crea un objeto StandardScaler para estandarizar las variables (media 0, varianza 1)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Ajusta y transforma los datos de entrenamiento</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Transforma los datos de prueba con la misma escala (sin volver a ajustar)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># CREACIÓN Y ENTRENAMIENTO DE MODELOS</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Define el modelo Ridge con penalización L2 (alpha = 1.0)</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Define el modelo Lasso con penalización L1 (alpha = 5.0)</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>

<span class="c1"># Define la regresión lineal sin penalización (modelo base OLS)</span>
<span class="n">regOLS</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Ajusta los modelos con los datos de entrenamiento escalados</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">regOLS</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># PREDICCIONES Y EVALUACIÓN EN TRAINING SET</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Realiza predicciones sobre el conjunto de entrenamiento</span>
<span class="n">y_pred_ridge_tr</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">y_pred_lasso_tr</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">y_pred_ols_tr</span> <span class="o">=</span> <span class="n">regOLS</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>

<span class="c1"># Calcula el error cuadrático medio (MSE) sobre el set de entrenamiento</span>
<span class="c1"># num_factor se puede usar si se quiere normalizar el error (aquí vale 1)</span>
<span class="n">num_factor</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mse_ridge_tr</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_ridge_tr</span><span class="p">)</span><span class="o">/</span><span class="n">num_factor</span>
<span class="n">mse_lasso_tr</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_lasso_tr</span><span class="p">)</span><span class="o">/</span><span class="n">num_factor</span>
<span class="n">mse_ols_tr</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_ols_tr</span><span class="p">)</span><span class="o">/</span><span class="n">num_factor</span>

<span class="c1"># Imprime los resultados de entrenamiento</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluación del modelo en TRAIN:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Ridge: </span><span class="si">{</span><span class="n">mse_ridge_tr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Lasso: </span><span class="si">{</span><span class="n">mse_lasso_tr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE OLS: </span><span class="si">{</span><span class="n">mse_ols_tr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># PREDICCIONES Y EVALUACIÓN EN TEST SET</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Realiza predicciones sobre el conjunto de prueba</span>
<span class="n">y_pred_ridge</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_lasso</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">regOLS</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># Calcula el MSE sobre el set de prueba</span>
<span class="n">mse_ridge</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">)</span><span class="o">/</span><span class="n">num_factor</span>
<span class="n">mse_lasso</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lasso</span><span class="p">)</span><span class="o">/</span><span class="n">num_factor</span>
<span class="n">mse_ols</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ols</span><span class="p">)</span><span class="o">/</span><span class="n">num_factor</span>

<span class="c1"># Imprime los resultados del test</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluación del modelo en TEST:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Ridge: </span><span class="si">{</span><span class="n">mse_ridge</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Lasso: </span><span class="si">{</span><span class="n">mse_lasso</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE OLS: </span><span class="si">{</span><span class="n">mse_ols</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluación del modelo en TRAIN:

MSE Ridge: 0.03
MSE Lasso: 0.25
MSE OLS: 0.00

Evaluación del modelo en TEST:

MSE Ridge: 0.69
MSE Lasso: 0.25
MSE OLS: 1.00
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Mostrar coeficientes con nombres</span>
<span class="n">ridge_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Ridge Coefficients&#39;</span><span class="p">])</span>
<span class="n">lasso_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Lasso Coefficients&#39;</span><span class="p">])</span>
<span class="n">ols_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">regOLS</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;OLS Coefficients&#39;</span><span class="p">])</span>

<span class="c1"># Combinar coeficientes en un solo DataFrame</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ridge_coefficients</span><span class="p">,</span> <span class="n">lasso_coefficients</span><span class="p">,</span><span class="n">ols_coefficients</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Coeficientes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">13</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Mostrar coeficientes con nombres</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">ridge_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Ridge Coefficients&#39;</span><span class="p">])</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">lasso_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Lasso Coefficients&#39;</span><span class="p">])</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">ols_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">regOLS</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;OLS Coefficients&#39;</span><span class="p">])</span>

<span class="ne">AttributeError</span>: &#39;numpy.ndarray&#39; object has no attribute &#39;columns&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Graficar los resultados</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicciones Ridge&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lasso</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicciones Lasso&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_test</span><span class="p">)],</span> <span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_test</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Línea de referencia&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valores reales&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Valores predichos&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparación de Predicciones: Ridge vs Lasso&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f688226add0a0eb593ec958750feea235ed5ca2ae3e04c079cd98049ec8f7bca.png" src="_images/f688226add0a0eb593ec958750feea235ed5ca2ae3e04c079cd98049ec8f7bca.png" />
</div>
</div>
<p>Ajuste con el mejor <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importación de herramientas para validación cruzada y búsqueda de hiperparámetros</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># DEFINICIÓN DEL RANGO DE ALPHA (λ) A PROBAR</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Define una grilla de valores posibles para el hiperparámetro alpha (lambda)</span>
<span class="c1"># Alpha controla la intensidad de la penalización (regularización)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">]}</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># CREACIÓN DE LOS MODELOS BASE</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Crea instancias de los modelos Ridge y Lasso (aún sin alpha especificado)</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">()</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># CONFIGURACIÓN DE LA VALIDACIÓN CRUZADA</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Crea un objeto KFold con 5 particiones (folds), barajando los datos y fijando semilla para reproducibilidad</span>
<span class="n">kfold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># BÚSQUEDA DE HIPERPARÁMETROS CON GRIDSEARCHCV</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Aplica GridSearchCV al modelo Ridge:</span>
<span class="c1"># - Se evalúa cada valor de alpha usando validación cruzada con el criterio de error cuadrático medio negativo</span>
<span class="c1">#   (porque sklearn maximiza por defecto, y el MSE debe minimizarse)</span>
<span class="n">ridge_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">ridge</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span>
<span class="p">)</span>
<span class="n">ridge_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># Entrena sobre los datos de entrenamiento escalados</span>

<span class="c1"># Aplica el mismo procedimiento para el modelo Lasso</span>
<span class="n">lasso_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">lasso</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span>
<span class="p">)</span>
<span class="n">lasso_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># RESULTADOS ÓPTIMOS DE RIDGE</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Extrae el mejor valor de alpha para Ridge que minimiza el MSE</span>
<span class="n">best_ridge_alpha</span> <span class="o">=</span> <span class="n">ridge_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>

<span class="c1"># Obtiene el mejor score (negativo), y lo multiplica por -1 para obtener el MSE real</span>
<span class="n">best_ridge_mse</span> <span class="o">=</span> <span class="o">-</span><span class="n">ridge_search</span><span class="o">.</span><span class="n">best_score_</span> <span class="o">/</span> <span class="n">num_factor</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># RESULTADOS ÓPTIMOS DE LASSO</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Extrae el mejor valor de alpha para Lasso</span>
<span class="n">best_lasso_alpha</span> <span class="o">=</span> <span class="n">lasso_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>

<span class="c1"># MSE asociado al mejor alpha encontrado para Lasso (también se vuelve positivo)</span>
<span class="n">best_lasso_mse</span> <span class="o">=</span> <span class="o">-</span><span class="n">lasso_search</span><span class="o">.</span><span class="n">best_score_</span> <span class="o">/</span> <span class="n">num_factor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importación de bibliotecas para visualización y operaciones numéricas</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># EXTRACCIÓN DE RESULTADOS DEL GRIDSEARCH</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Accede a los resultados de validación cruzada del GridSearch para Ridge</span>
<span class="n">ridge_results</span> <span class="o">=</span> <span class="n">ridge_search</span><span class="o">.</span><span class="n">cv_results_</span>

<span class="c1"># Accede a los resultados de validación cruzada del GridSearch para Lasso</span>
<span class="n">lasso_results</span> <span class="o">=</span> <span class="n">lasso_search</span><span class="o">.</span><span class="n">cv_results_</span>

<span class="c1"># Extrae los valores de alpha evaluados</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">ridge_results</span><span class="p">[</span><span class="s1">&#39;param_alpha&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># Asegura que esté en formato iterable limpio</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># CÁLCULO DE MSE (recordando que estaban como negativos)</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Convierte los scores negativos de Ridge a MSE positivos</span>
<span class="n">ridge_mse</span> <span class="o">=</span> <span class="o">-</span><span class="n">ridge_results</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span>

<span class="c1"># Convierte los scores negativos de Lasso a MSE positivos</span>
<span class="n">lasso_mse</span> <span class="o">=</span> <span class="o">-</span><span class="n">lasso_results</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># VISUALIZACIÓN DE LOS RESULTADOS</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Crea una figura de tamaño amplio</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="c1"># ----------- SUBPLOT PARA RIDGE -----------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Primer gráfico (1 fila, 2 columnas, posición 1)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">ridge_mse</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MSE Ridge&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>  <span class="c1"># Curva MSE vs alpha</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">best_ridge_alpha</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Best alpha: </span><span class="si">{</span><span class="n">best_ridge_alpha</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># Línea en el mejor alpha</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>  <span class="c1"># Usa escala logarítmica en el eje x</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Ridge Regression: MSE vs. Alpha&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha (log scale)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ----------- SUBPLOT PARA LASSO -----------</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Segundo gráfico (1 fila, 2 columnas, posición 2)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">lasso_mse</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MSE Lasso&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>  <span class="c1"># Curva MSE vs alpha</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">best_lasso_alpha</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Best alpha: </span><span class="si">{</span><span class="n">best_lasso_alpha</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># Línea en el mejor alpha</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>  <span class="c1"># Escala logarítmica para visualizar bien los valores pequeños</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Lasso Regression: MSE vs. Alpha&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha (log scale)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Ajusta automáticamente el diseño para que no se sobrepongan los elementos</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c1"># Muestra el gráfico final con ambos subplots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d0da2a598c864b1f1bab5f924a79adaf1fd51e97b00cfc501b0374f75a0691c7.png" src="_images/d0da2a598c864b1f1bab5f924a79adaf1fd51e97b00cfc501b0374f75a0691c7.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># IMPRESIÓN DE LOS MEJORES RESULTADOS DE VALIDACIÓN CRUZADA</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Imprime los mejores valores encontrados en la validación cruzada para ambos modelos</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluación del modelo en CV:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mejor alpha para Ridge: </span><span class="si">{</span><span class="n">best_ridge_alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mejor MSE para Ridge: </span><span class="si">{</span><span class="n">best_ridge_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mejor alpha para Lasso: </span><span class="si">{</span><span class="n">best_lasso_alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mejor MSE para Lasso: </span><span class="si">{</span><span class="n">best_lasso_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># AJUSTAR LOS MODELOS CON LOS MEJORES ALPHA</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Crea nuevas instancias de Ridge y Lasso con los mejores valores de alpha</span>
<span class="n">ridge_best</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">best_ridge_alpha</span><span class="p">)</span>
<span class="n">lasso_best</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">best_lasso_alpha</span><span class="p">)</span>

<span class="c1"># Ajusta ambos modelos con los datos de entrenamiento escalados</span>
<span class="n">ridge_best</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lasso_best</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># PREDICCIONES Y MSE EN TRAINING SET</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Realiza predicciones sobre el conjunto de entrenamiento usando los modelos ajustados con el mejor alpha</span>
<span class="n">y_pred_ridge_best_tr</span> <span class="o">=</span> <span class="n">ridge_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">y_pred_lasso_best_tr</span> <span class="o">=</span> <span class="n">lasso_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>

<span class="c1"># Calcula el MSE en el conjunto de entrenamiento</span>
<span class="n">mse_ridge_best_tr</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_ridge_best_tr</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_factor</span>
<span class="n">mse_lasso_best_tr</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_lasso_best_tr</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_factor</span>

<span class="c1"># Imprime los MSE obtenidos en entrenamiento</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluación del modelo en TRAIN:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MSE Ridge con mejor alpha: </span><span class="si">{</span><span class="n">mse_ridge_best_tr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Lasso con mejor alpha: </span><span class="si">{</span><span class="n">mse_lasso_best_tr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>

<span class="c1"># -------------------------------------------------------------</span>
<span class="c1"># PREDICCIONES Y MSE EN TEST SET</span>
<span class="c1"># -------------------------------------------------------------</span>

<span class="c1"># Realiza predicciones sobre el conjunto de prueba usando los modelos ajustados con el mejor alpha</span>
<span class="n">y_pred_ridge_best</span> <span class="o">=</span> <span class="n">ridge_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_lasso_best</span> <span class="o">=</span> <span class="n">lasso_best</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># Calcula el MSE en el conjunto de prueba</span>
<span class="n">mse_ridge_best</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ridge_best</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_factor</span>
<span class="n">mse_lasso_best</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lasso_best</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_factor</span>

<span class="c1"># Imprime los MSE obtenidos en prueba (test)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluación del modelo en TEST:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MSE Ridge con mejor alpha: </span><span class="si">{</span><span class="n">mse_ridge_best</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Lasso con mejor alpha: </span><span class="si">{</span><span class="n">mse_lasso_best</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluación del modelo en CV:

Mejor alpha para Ridge: 300
Mejor MSE para Ridge: 2282.32 
Mejor alpha para Lasso: 0.9
Mejor MSE para Lasso: 2374.17 

Evaluación del modelo en TRAIN:


MSE Ridge con mejor alpha: 1903.47
MSE Lasso con mejor alpha: 1871.67 

Evaluación del modelo en TEST:


MSE Ridge con mejor alpha: 3617.36
MSE Lasso con mejor alpha: 3534.08 
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-base-py"
        },
        kernelOptions: {
            name: "conda-base-py",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-base-py'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ProbModels.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Modelos de Probabilidad</p>
      </div>
    </a>
    <a class="right-next"
       href="Arboles.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Árboles de decisión</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-variables">10.1. Selección de variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-las-mejores-variables">10.1.1. Selección de las mejores variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-adelante">10.1.2. Selección <em>stepwise</em> hacia adelante</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-atras">10.1.3. Selección <em>stepwise</em> hacia atrás</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo">10.1.3.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizacion">10.2. Regularización</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion">10.2.1. Introducción</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">10.2.1.1. Ejemplo</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#un-contexto-mas-general">10.2.2. Un contexto más general</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge">10.2.3. Regresión <em>Ridge</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lasso">10.2.4. Regresión <em>Lasso</em></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">10.2.4.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Víctor Morales Oñate
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>