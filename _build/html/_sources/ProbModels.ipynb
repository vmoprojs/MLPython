{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Probabilidad\n",
    "\n",
    "## Probabilidad lineal\n",
    "\n",
    "En este caso la variable dependiente es una dummy\n",
    "\n",
    "\n",
    "<img src=\"images/RL_Im7.png\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata de modelos del tipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{i} = \\beta_{0}+\\beta_{1}X_{1i}+u_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo: Abrir la base `MROZ` de Wooldridge y ajuste el modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "inlf=\\beta_0nwifeinc+\\beta_1educ + \\beta_2exper +\\beta_3expersq + \\beta_4age + \\beta_5kidslt6 + \\beta_6kidsge6\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   inlf   R-squared:                       0.264\n",
      "Model:                            OLS   Adj. R-squared:                  0.257\n",
      "Method:                 Least Squares   F-statistic:                     38.22\n",
      "Date:                Wed, 18 Sep 2024   Prob (F-statistic):           6.90e-46\n",
      "Time:                        05:33:31   Log-Likelihood:                -423.89\n",
      "No. Observations:                 753   AIC:                             863.8\n",
      "Df Residuals:                     745   BIC:                             900.8\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.5855      0.154      3.798      0.000       0.283       0.888\n",
      "nwifeinc      -0.0034      0.001     -2.351      0.019      -0.006      -0.001\n",
      "educ           0.0380      0.007      5.151      0.000       0.024       0.052\n",
      "exper          0.0395      0.006      6.962      0.000       0.028       0.051\n",
      "expersq       -0.0006      0.000     -3.227      0.001      -0.001      -0.000\n",
      "age           -0.0161      0.002     -6.476      0.000      -0.021      -0.011\n",
      "kidslt6       -0.2618      0.034     -7.814      0.000      -0.328      -0.196\n",
      "kidsge6        0.0130      0.013      0.986      0.324      -0.013       0.039\n",
      "==============================================================================\n",
      "Omnibus:                      169.137   Durbin-Watson:                   0.494\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               36.741\n",
      "Skew:                          -0.196   Prob(JB):                     1.05e-08\n",
      "Kurtosis:                       1.991   Cond. No.                     3.06e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.06e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "uu = \"https://raw.githubusercontent.com/vmoprojs/DataLectures/master/WO/mroz.csv\"\n",
    "datos = pd.read_csv(uu,header = None)\n",
    "\n",
    "\n",
    "datos.columns = [\"inlf\",\"hours\",  \"kidslt6\", \"kidsge6\", \n",
    "               \"age\", \"educ\",  \"wage\",\n",
    "               \"repwage\",\"hushrs\"  ,  \"husage\", \"huseduc\" ,\n",
    "               \"huswage\"  , \"faminc\", \"mtr\",\"motheduc\",\n",
    "               \"fatheduc\" , \"unem\",  \"city\" , \"exper\" , \n",
    "               \"nwifeinc\" , \"lwage\" ,\"expersq\"]\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "from scipy import stats\n",
    "\n",
    "reg1 = smf.ols('inlf ~ nwifeinc + educ + exper + expersq + age+kidslt6 + kidsge6',data = datos).fit()\n",
    "print(reg1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué hemos ajustado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux = smf.ols('inlf ~ educ',data = datos).fit()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(datos.educ,datos.inlf,'o');\n",
    "plt.plot(datos.educ,aux.fittedvalues,'-',color='r');\n",
    "plt.xlabel('educ');\n",
    "plt.ylabel('inlf');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Excepto *kidsge6* los coeficientes son significativos.\n",
    "- Se introdujo la experiencia cuadrática para capturar un efecto decreciente en el efecto deseado (`inlf`). ¿Cómo lo interpretamos?\n",
    "\n",
    "`.039 - 2(.0006)exper = 0.39 - .0012exper`\n",
    "\n",
    "- El punto en el que la experiencia ya no tiene efecto en `inlf` es $.039/.0012 = 32.5$. ¿Cuantos elementos de la muestra tienen más de 32 años de experiencia?\n",
    "\n",
    "\n",
    "Se añade exper al cuadrado porque queremos dar la posibilidad que los años adicionales de expericnecia contribuyan con un efecto decreciente.\n",
    "\n",
    "Trabajemos ahora con la predicción, y revisemos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.6636123221355517), (1, 0.7009165727274149), (1, 0.6727286212890473), (1, 0.7257441305286598), (1, 0.5616358247349595)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=753, minmax=(-0.3451102646574087, 1.1271505290421102), mean=0.5683930942895058, variance=0.06490433214015537, skewness=-0.4241251818053438, kurtosis=-0.07391834122184227)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals = reg1.predict()\n",
    "aux = list(zip(datos.inlf,pred_vals))\n",
    "print(aux[0:5])\n",
    "\n",
    "\n",
    "stats.describe(pred_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos notar?\n",
    "\n",
    "\n",
    "- Existen valores mayores a 1 e inferiores a 0.\n",
    "- $R^{2}$ ya no es interpretable en estas regresiones.\n",
    "- Usaremos una probabilidad de ocurrencia, digamos 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343957503320053"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion_dum = (pred_vals>=0.5)*1\n",
    "\n",
    "tab = pd.crosstab(datos.inlf,prediccion_dum)\n",
    "(tab.iloc[0,0]+tab.iloc[1,1])/datos.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "La regresión logística puede entenderse simplemente como encontrar los parámtros $\\beta$ que mejor asjuten:\n",
    "\n",
    "$$\n",
    "y={\\begin{cases}1&\\beta_{1}+\\beta_{2}X_{1}+\\cdots+\\beta_{k}X_{k}+u >0\\\\0&{\\text{en otro caso}}\\end{cases}}\n",
    "$$\n",
    "\n",
    "Donde se asume que el error tiene una [distribución logística estándar](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_log%C3%ADstica)\n",
    "\n",
    "$$\n",
    "{\\displaystyle f(x;\\mu ,s)={\\frac {e^{-{\\frac {x-\\mu }{s}}}}{s\\left(1+e^{-{\\frac {x-\\mu }{s}}}\\right)^{2}}}={\\frac {1}{s\\left(e^{\\frac {x-\\mu }{2s}}+e^{-{\\frac {x-\\mu }{2s}}}\\right)^{2}}}={\\frac {1}{4s}}\\operatorname {sech} ^{2}\\!\\left({\\frac {x-\\mu }{2s}}\\right).}\n",
    "$$\n",
    "\n",
    "Donde $s$ es el parámetro de escala y $\\mu$ el de locación (*sech* es la función secante hiperbólico).\n",
    "\n",
    "\n",
    "Otra forma de entender la regresión logística es a través de la función logística:\n",
    "\n",
    "$$\n",
    "\\sigma (t)={\\frac {e^{t}}{e^{t}+1}}={\\frac {1}{1+e^{-t}}}\n",
    "$$\n",
    "\n",
    "donde $t\\in \\mathbb{R}$ y $0\\leq\\sigma (t)\\leq1$.\n",
    "\n",
    "Asumiento $t$ como una función lineal de una variable explicativa $x$, tenemos:\n",
    "\n",
    "$$\n",
    "t=\\beta _{0}+\\beta _{1}x\n",
    "$$\n",
    "\n",
    "Ahora la función logística se puede expresar:\n",
    "\n",
    "$$\n",
    "p(x)={\\frac {1}{1+e^{-(\\beta _{0}+\\beta _{1}x)}}}\n",
    "$$\n",
    "\n",
    "Ten en cuenta que $p (x)$ se interpreta como la probabilidad de que la variable dependiente iguale a *éxito*  en lugar de un *fracaso*. Está claro que las variables de respuesta $Y_ {i}$ no se distribuyen de forma idéntica: $ P (Y_ {i} = 1 \\ mid X )$ difiere de un punto $X_ {i}$ a otro, aunque son independientes dado que la matriz de diseño $X$ y los parámetros compartidos $\\beta$.\n",
    "\n",
    "Finalmente definimos la inversa de la función logística, $g$, el **logit** (log odds):\n",
    "\n",
    "$$\n",
    "{\\displaystyle g(p(x))=\\ln \\left({\\frac {p(x)}{1-p(x)}}\\right)=\\beta _{0}+\\beta _{1}x,}\n",
    "$$\n",
    "\n",
    "lo que es equivalente a:\n",
    "\n",
    "$$\n",
    "{\\frac {p(x)}{1-p(x)}}=e^{\\beta _{0}+\\beta _{1}x}\n",
    "$$\n",
    "\n",
    "**Interpretación**:\n",
    "\n",
    "-   $g$ es la función logit. La ecuación para $g (p (x))$ ilustra que el logit (es decir, log-odds o logaritmo natural de las probabilidades) es equivalente a la expresión de regresión lineal.\n",
    "-   $ln$ denota el logaritmo natural.\n",
    "-   $p (x)$ es la probabilidad de que la variable dependiente sea igual a un caso, dada alguna combinación lineal de los predictores. La fórmula para $p (x)$ ilustra que la probabilidad de que la variable dependiente iguale un caso es igual al valor de la función logística de la expresión de regresión lineal. Esto es importante porque muestra que el valor de la expresión de regresión lineal puede variar de infinito negativo a positivo y, sin embargo, después de la transformación, la expresión resultante para la probabilidad $p (x)$ oscila entre $0$ y $1$.\n",
    "-   $\\beta _ {0}$ es la intersección de la ecuación de regresión lineal (el valor del criterio cuando el predictor es igual a cero).\n",
    "-   $\\beta _ {1} x$ es el coeficiente de regresión multiplicado por algún valor del predictor.\n",
    "-   la base $e$ denota la función exponencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Abra la tabla 15.7 \n",
    "\n",
    "\n",
    "-   Los datos son el efecto del Sistema de Enseñanza Personalizada (PSI) sobre las calificaciones.\n",
    "    -   Calificación $Y = 1$ si la calificación final fue A\n",
    "    -   $Y = 0$ si la calificación final fue B o C\n",
    "    -   `TUCE` = calificación en un examen presentado al comienzo del curso para evaluar los conocimientos previos de macroeconomía\n",
    "    -   `PSI` = 1 con el nuevo método de enseñanza, 0 en otro caso\n",
    "    -   `GPA` = promedio de puntos de calificación inicial\n",
    "-   Ajuste el siguiente modelo:\n",
    "`ajuste1 <- glm(GRADE~GPA+TUCE+PSI,\n",
    "family=binomial(link=\"logit\"),x=T)`\n",
    "-   Interprete el modelo\n",
    "\n",
    "\n",
    "En los modelos cuya variable regresada binaria, la bondad del ajuste tiene una importancia secundaria. Lo que interesa son los signos esperados de los coeficientes de la regresión y su importancia práctica y/o estadística.\n",
    "\n",
    "\n",
    "Importamos los datos y revisamos la variable dependiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>GPA</th>\n",
       "      <th>LETTER</th>\n",
       "      <th>OBS</th>\n",
       "      <th>PSI</th>\n",
       "      <th>TUCE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRADE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      len                    \n",
       "      GPA LETTER OBS PSI TUCE\n",
       "GRADE                        \n",
       "0      21     21  21  21   21\n",
       "1      11     11  11  11   11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uu = \"https://raw.githubusercontent.com/vmoprojs/DataLectures/master/GA/tabla15_7.csv\"\n",
    "datos = pd.read_csv(uu,sep = ';')\n",
    "datos.columns\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "datos.pivot_table(index = 'GRADE', aggfunc = [len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.402801\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   No. Observations:                   32\n",
      "Model:                          Logit   Df Residuals:                       28\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Wed, 18 Sep 2024   Pseudo R-squ.:                  0.3740\n",
      "Time:                        05:33:32   Log-Likelihood:                -12.890\n",
      "converged:                       True   LL-Null:                       -20.592\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.001502\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -13.0213      4.931     -2.641      0.008     -22.687      -3.356\n",
      "GPA            2.8261      1.263      2.238      0.025       0.351       5.301\n",
      "TUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\n",
      "PSI            2.3787      1.065      2.234      0.025       0.292       4.465\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "ajuste1 = smf.logit('GRADE ~ GPA + TUCE + PSI',data = datos).fit()\n",
    "\n",
    "print(ajuste1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Marginal Effects</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th> <td>GRADE</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>        <td>dydx</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>At:</th>            <td>mean</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <th></th>      <th>dy/dx</th>    <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>  <td>    0.5339</td> <td>    0.237</td> <td>    2.252</td> <td> 0.024</td> <td>    0.069</td> <td>    0.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TUCE</th> <td>    0.0180</td> <td>    0.026</td> <td>    0.685</td> <td> 0.493</td> <td>   -0.033</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PSI</th>  <td>    0.4493</td> <td>    0.197</td> <td>    2.284</td> <td> 0.022</td> <td>    0.064</td> <td>    0.835</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:} &     GRADE       \\\\\n",
       "\\textbf{Method:}        &      dydx       \\\\\n",
       "\\textbf{At:}            &      mean       \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "  \\textbf{}   & \\textbf{dy/dx} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{GPA}  &       0.5339   &        0.237     &     2.252  &         0.024        &        0.069    &        0.998     \\\\\n",
       "\\textbf{TUCE} &       0.0180   &        0.026     &     0.685  &         0.493        &       -0.033    &        0.069     \\\\\n",
       "\\textbf{PSI}  &       0.4493   &        0.197     &     2.284  &         0.022        &        0.064    &        0.835     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Marginal Effects}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "        Logit Marginal Effects       \n",
       "=====================================\n",
       "Dep. Variable:                  GRADE\n",
       "Method:                          dydx\n",
       "At:                              mean\n",
       "==============================================================================\n",
       "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "GPA            0.5339      0.237      2.252      0.024       0.069       0.998\n",
       "TUCE           0.0180      0.026      0.685      0.493      -0.033       0.069\n",
       "PSI            0.4493      0.197      2.284      0.022       0.064       0.835\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.get_margeff(at='mean').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07311607, 0.00246188, 0.06154047],\n",
       "       [0.15815168, 0.0053251 , 0.1331134 ],\n",
       "       [0.43011639, 0.01448239, 0.36202116],\n",
       "       [0.07130492, 0.0024009 , 0.06001605],\n",
       "       [0.69272252, 0.02332457, 0.58305197],\n",
       "       [0.09507939, 0.00320141, 0.08002659],\n",
       "       [0.0729182 , 0.00245522, 0.06137393],\n",
       "       [0.1381988 , 0.00465327, 0.11631942],\n",
       "       [0.2791564 , 0.00939944, 0.23496087],\n",
       "       [0.60069976, 0.02022608, 0.50559808]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = ajuste1.get_margeff(at='all').margeff\n",
    "mm[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Son, en conjunto, los coeficientes significativos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<Wald test (chi2): statistic=8.873128862019607, p-value=0.06435007959304212, df_denom=4>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp = '(Intercept = 0, GPA = 0,TUCE=0,PSI=0)'\n",
    "ajuste1.wald_test(hyp,scalar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept     0.000002\n",
       "GPA          16.879715\n",
       "TUCE          1.099832\n",
       "PSI          10.790732\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(ajuste1.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto indica que los estudiantes expuestos al nuevo método de enseñanza son 10 veces más propensos a obtener una A que quienes no están expuestos al nuevo método, en tanto no cambien los demás factores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.09308603, -1.61569183, -0.87816803, -2.08420699,  0.13722837,\n",
       "       -1.92311086, -2.08569187, -1.69993698, -1.23289159,  0.42099513,\n",
       "       -2.14186033, -0.86486457, -0.45841206, -0.85895528, -0.36825763,\n",
       "       -2.01475018, -1.6881184 , -1.86842727,  0.23630576,  0.40479606,\n",
       "       -1.53878182,  1.30785551, -0.60319106,  1.02555852,  0.97087493,\n",
       "       -0.02826337,  0.3649008 , -0.44357981,  0.99452719,  1.6670186 ,\n",
       "        0.10033167, -1.15745135])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.predict(which = 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probit\n",
    "\n",
    "En los modelos logia se propuso la logística, en este caso se propone la Función de Distribución Acumulada Normal. Suponga que la variable de respuesta es binaria, 1 o 0. $Y$ podría representar la presencia/ausencia de una condición, éxito/fracaso, si/no. Se tiene también un vector de regresoras $X$, el modelo toma la forma:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\Pr(Y=1\\mid X)=\\Phi (X^{T}\\beta ),}\n",
    "$$\n",
    "\n",
    "donde $Pr$ es la prbabilidad y $\\Phi$ distribución acumulada de la normal estándar ${\\displaystyle \\Phi (x)={\\frac {1}{\\sqrt {2\\pi }}}\\int _{-\\infty }^{x}e^{-t^{2}/2}\\,dt}$. Los parámetros $\\beta$ se estiman típicamente con el método de máxima verosimilitud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.400588\n",
      "         Iterations 6\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   No. Observations:                   32\n",
      "Model:                         Probit   Df Residuals:                       28\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Wed, 18 Sep 2024   Pseudo R-squ.:                  0.3775\n",
      "Time:                        05:33:32   Log-Likelihood:                -12.819\n",
      "converged:                       True   LL-Null:                       -20.592\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.001405\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -7.4523      2.542     -2.931      0.003     -12.435      -2.469\n",
      "GPA            1.6258      0.694      2.343      0.019       0.266       2.986\n",
      "TUCE           0.0517      0.084      0.617      0.537      -0.113       0.216\n",
      "PSI            1.4263      0.595      2.397      0.017       0.260       2.593\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "ajuste1 = smf.probit('GRADE ~ GPA + TUCE + PSI',data = datos).fit()\n",
    "\n",
    "print(ajuste1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Marginal Effects</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th> <td>GRADE</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>        <td>dydx</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>At:</th>            <td>mean</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <th></th>      <th>dy/dx</th>    <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>  <td>    0.5333</td> <td>    0.232</td> <td>    2.294</td> <td> 0.022</td> <td>    0.078</td> <td>    0.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TUCE</th> <td>    0.0170</td> <td>    0.027</td> <td>    0.626</td> <td> 0.531</td> <td>   -0.036</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PSI</th>  <td>    0.4679</td> <td>    0.188</td> <td>    2.494</td> <td> 0.013</td> <td>    0.100</td> <td>    0.836</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:} &     GRADE       \\\\\n",
       "\\textbf{Method:}        &      dydx       \\\\\n",
       "\\textbf{At:}            &      mean       \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{ccccccc}\n",
       "  \\textbf{}   & \\textbf{dy/dx} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\textbf{GPA}  &       0.5333   &        0.232     &     2.294  &         0.022        &        0.078    &        0.989     \\\\\n",
       "\\textbf{TUCE} &       0.0170   &        0.027     &     0.626  &         0.531        &       -0.036    &        0.070     \\\\\n",
       "\\textbf{PSI}  &       0.4679   &        0.188     &     2.494  &         0.013        &        0.100    &        0.836     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Probit Marginal Effects}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "       Probit Marginal Effects       \n",
       "=====================================\n",
       "Dep. Variable:                  GRADE\n",
       "Method:                          dydx\n",
       "At:                              mean\n",
       "==============================================================================\n",
       "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "GPA            0.5333      0.232      2.294      0.022       0.078       0.989\n",
       "TUCE           0.0170      0.027      0.626      0.531      -0.036       0.070\n",
       "PSI            0.4679      0.188      2.494      0.013       0.100       0.836\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.get_margeff(at='mean').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07255307, 0.00230845, 0.06365122],\n",
       "       [0.17584323, 0.00559486, 0.15426826],\n",
       "       [0.44108314, 0.01403409, 0.38696473],\n",
       "       [0.07391114, 0.00235166, 0.06484266],\n",
       "       [0.64252589, 0.02044346, 0.5636916 ],\n",
       "       [0.10206861, 0.00324755, 0.08954537],\n",
       "       [0.07368267, 0.00234439, 0.06464222],\n",
       "       [0.15292232, 0.00486558, 0.13415961],\n",
       "       [0.3033272 , 0.00965106, 0.26611067],\n",
       "       [0.59359928, 0.01888675, 0.520768  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = ajuste1.get_margeff(at='all').margeff\n",
    "mm[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.09308603, -1.61569183, -0.87816803, -2.08420699,  0.13722837,\n",
       "       -1.92311086, -2.08569187, -1.69993698, -1.23289159,  0.42099513,\n",
       "       -2.14186033, -0.86486457, -0.45841206, -0.85895528, -0.36825763,\n",
       "       -2.01475018, -1.6881184 , -1.86842727,  0.23630576,  0.40479606,\n",
       "       -1.53878182,  1.30785551, -0.60319106,  1.02555852,  0.97087493,\n",
       "       -0.02826337,  0.3649008 , -0.44357981,  0.99452719,  1.6670186 ,\n",
       "        0.10033167, -1.15745135])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.predict(which = 'linear')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
