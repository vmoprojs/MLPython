{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Probabilidad\n",
    "\n",
    "## Probabilidad lineal\n",
    "\n",
    "En este caso la variable dependiente es una dummy\n",
    "\n",
    "\n",
    "<img src=\"images/RL_Im7.png\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata de modelos del tipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{i} = \\beta_{0}+\\beta_{1}X_{1i}+u_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo: Abrir la base `MROZ` de Wooldridge y ajuste el modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "inlf=\\beta_0nwifeinc+\\beta_1educ + \\beta_2exper +\\beta_3expersq + \\beta_4age + \\beta_5kidslt6 + \\beta_6kidsge6\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   inlf   R-squared:                       0.264\n",
      "Model:                            OLS   Adj. R-squared:                  0.257\n",
      "Method:                 Least Squares   F-statistic:                     38.22\n",
      "Date:                Tue, 28 Mar 2023   Prob (F-statistic):           6.90e-46\n",
      "Time:                        03:58:25   Log-Likelihood:                -423.89\n",
      "No. Observations:                 753   AIC:                             863.8\n",
      "Df Residuals:                     745   BIC:                             900.8\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.5855      0.154      3.798      0.000       0.283       0.888\n",
      "nwifeinc      -0.0034      0.001     -2.351      0.019      -0.006      -0.001\n",
      "educ           0.0380      0.007      5.151      0.000       0.024       0.052\n",
      "exper          0.0395      0.006      6.962      0.000       0.028       0.051\n",
      "expersq       -0.0006      0.000     -3.227      0.001      -0.001      -0.000\n",
      "age           -0.0161      0.002     -6.476      0.000      -0.021      -0.011\n",
      "kidslt6       -0.2618      0.034     -7.814      0.000      -0.328      -0.196\n",
      "kidsge6        0.0130      0.013      0.986      0.324      -0.013       0.039\n",
      "==============================================================================\n",
      "Omnibus:                      169.137   Durbin-Watson:                   0.494\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               36.741\n",
      "Skew:                          -0.196   Prob(JB):                     1.05e-08\n",
      "Kurtosis:                       1.991   Cond. No.                     3.06e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.06e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "uu = \"https://raw.githubusercontent.com/vmoprojs/DataLectures/master/WO/mroz.csv\"\n",
    "datos = pd.read_csv(uu,header = None)\n",
    "\n",
    "\n",
    "datos.columns = [\"inlf\",\"hours\",  \"kidslt6\", \"kidsge6\", \n",
    "               \"age\", \"educ\",  \"wage\",\n",
    "               \"repwage\",\"hushrs\"  ,  \"husage\", \"huseduc\" ,\n",
    "               \"huswage\"  , \"faminc\", \"mtr\",\"motheduc\",\n",
    "               \"fatheduc\" , \"unem\",  \"city\" , \"exper\" , \n",
    "               \"nwifeinc\" , \"lwage\" ,\"expersq\"]\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as stm\n",
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "from scipy import stats\n",
    "\n",
    "reg1 = stm.ols('inlf ~ nwifeinc + educ + exper + expersq + age+kidslt6 + kidsge6',data = datos).fit()\n",
    "print(reg1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué hemos ajustado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux = stm.ols('inlf ~ educ',data = datos).fit()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(datos.educ,datos.inlf,'o');\n",
    "plt.plot(datos.educ,aux.fittedvalues,'-',color='r');\n",
    "plt.xlabel('educ');\n",
    "plt.ylabel('inlf');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Excepto *kidsge6* los coeficientes son significativos.\n",
    "- Se introdujo la experiencia cuadrática para capturar un efecto decreciente en el efecto deseado (`inlf`). ¿Cómo lo interpretamos?\n",
    "\n",
    "`.039 - 2(.0006)exper = 0.39 - .0012exper`\n",
    "\n",
    "- El punto en el que la experiencia ya no tiene efecto en `inlf` es $.039/.0012 = 32.5$. ¿Cuantos elementos de la muestra tienen más de 32 años de experiencia?\n",
    "\n",
    "\n",
    "Se añade exper al cuadrado porque queremos dar la posibilidad que los años adicionales de expericnecia contribuyan con un efecto decreciente.\n",
    "\n",
    "Trabajemos ahora con la predicción, y revisemos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.6636123221355531), (1, 0.7009165727274161), (1, 0.6727286212890489), (1, 0.7257441305286613), (1, 0.5616358247349608)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=753, minmax=(-0.34511026465740724, 1.1271505290421115), mean=0.5683930942895072, variance=0.06490433214015544, skewness=-0.4241251818053419, kurtosis=-0.07391834122184537)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals = reg1.predict()\n",
    "aux = list(zip(datos.inlf,pred_vals))\n",
    "print(aux[0:5])\n",
    "\n",
    "\n",
    "stats.describe(pred_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos notar?\n",
    "\n",
    "\n",
    "- Existen valores mayores a 1 e inferiores a 0.\n",
    "- $R^{2}$ ya no es interpretable en estas regresiones.\n",
    "- Usaremos una probabilidad de ocurrencia, digamos 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343957503320053"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion_dum = (pred_vals>=0.5)*1\n",
    "\n",
    "tab = pd.crosstab(datos.inlf,prediccion_dum)\n",
    "(tab.iloc[0,0]+tab.iloc[1,1])/datos.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "La regresión logística puede entenderse simplemente como encontrar los parámtros $\\beta$ que mejor asjuten:\n",
    "\n",
    "$$\n",
    "y={\\begin{cases}1&\\beta_{1}+\\beta_{2}X_{1}+\\cdots+\\beta_{k}X_{k}+u >0\\\\0&{\\text{en otro caso}}\\end{cases}}\n",
    "$$\n",
    "\n",
    "Donde se asume que el error tiene una [distribución logística estándar](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_log%C3%ADstica)\n",
    "\n",
    "$$\n",
    "{\\displaystyle f(x;\\mu ,s)={\\frac {e^{-{\\frac {x-\\mu }{s}}}}{s\\left(1+e^{-{\\frac {x-\\mu }{s}}}\\right)^{2}}}={\\frac {1}{s\\left(e^{\\frac {x-\\mu }{2s}}+e^{-{\\frac {x-\\mu }{2s}}}\\right)^{2}}}={\\frac {1}{4s}}\\operatorname {sech} ^{2}\\!\\left({\\frac {x-\\mu }{2s}}\\right).}\n",
    "$$\n",
    "\n",
    "Donde $s$ es el parámetro de escala y $\\mu$ el de locación (*sech* es la función secante hiperbólico).\n",
    "\n",
    "\n",
    "Otra forma de entender la regresión logística es a través de la función logística:\n",
    "\n",
    "$$\n",
    "\\sigma (t)={\\frac {e^{t}}{e^{t}+1}}={\\frac {1}{1+e^{-t}}}\n",
    "$$\n",
    "\n",
    "donde $t\\in \\mathbb{R}$ y $0\\leq\\sigma (t)\\leq1$.\n",
    "\n",
    "Asumiento $t$ como una función lineal de una variable explicativa $x$, tenemos:\n",
    "\n",
    "$$\n",
    "t=\\beta _{0}+\\beta _{1}x\n",
    "$$\n",
    "\n",
    "Ahora la función logística se puede expresar:\n",
    "\n",
    "$$\n",
    "p(x)={\\frac {1}{1+e^{-(\\beta _{0}+\\beta _{1}x)}}}\n",
    "$$\n",
    "\n",
    "Ten en cuenta que $p (x)$ se interpreta como la probabilidad de que la variable dependiente iguale a *éxito*  en lugar de un *fracaso*. Está claro que las variables de respuesta $Y_ {i}$ no se distribuyen de forma idéntica: $ P (Y_ {i} = 1 \\ mid X )$ difiere de un punto $X_ {i}$ a otro, aunque son independientes dado que la matriz de diseño $X$ y los parámetros compartidos $\\beta$.\n",
    "\n",
    "Finalmente definimos la inversa de la función logística, $g$, el **logit** (log odds):\n",
    "\n",
    "$$\n",
    "{\\displaystyle g(p(x))=\\ln \\left({\\frac {p(x)}{1-p(x)}}\\right)=\\beta _{0}+\\beta _{1}x,}\n",
    "$$\n",
    "\n",
    "lo que es equivalente a:\n",
    "\n",
    "$$\n",
    "{\\frac {p(x)}{1-p(x)}}=e^{\\beta _{0}+\\beta _{1}x}\n",
    "$$\n",
    "\n",
    "**Interpretación**:\n",
    "\n",
    "-   $g$ es la función logit. La ecuación para $g (p (x))$ ilustra que el logit (es decir, log-odds o logaritmo natural de las probabilidades) es equivalente a la expresión de regresión lineal.\n",
    "-   $ln$ denota el logaritmo natural.\n",
    "-   $p (x)$ es la probabilidad de que la variable dependiente sea igual a un caso, dada alguna combinación lineal de los predictores. La fórmula para $p (x)$ ilustra que la probabilidad de que la variable dependiente iguale un caso es igual al valor de la función logística de la expresión de regresión lineal. Esto es importante porque muestra que el valor de la expresión de regresión lineal puede variar de infinito negativo a positivo y, sin embargo, después de la transformación, la expresión resultante para la probabilidad $p (x)$ oscila entre $0$ y $1$.\n",
    "-   $\\beta _ {0}$ es la intersección de la ecuación de regresión lineal (el valor del criterio cuando el predictor es igual a cero).\n",
    "-   $\\beta _ {1} x$ es el coeficiente de regresión multiplicado por algún valor del predictor.\n",
    "-   la base $e$ denota la función exponencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Abra la tabla 15.7 \n",
    "\n",
    "\n",
    "-   Los datos son el efecto del Sistema de Enseñanza Personalizada (PSI) sobre las calificaciones.\n",
    "    -   Calificación $Y = 1$ si la calificación final fue A\n",
    "    -   $Y = 0$ si la calificación final fue B o C\n",
    "    -   `TUCE` = calificación en un examen presentado al comienzo del curso para evaluar los conocimientos previos de macroeconomía\n",
    "    -   `PSI` = 1 con el nuevo método de enseñanza, 0 en otro caso\n",
    "    -   `GPA` = promedio de puntos de calificación inicial\n",
    "-   Ajuste el siguiente modelo:\n",
    "`ajuste1 <- glm(GRADE~GPA+TUCE+PSI,\n",
    "family=binomial(link=\"logit\"),x=T)`\n",
    "-   Interprete el modelo\n",
    "\n",
    "\n",
    "En los modelos cuya variable regresada binaria, la bondad del ajuste tiene una importancia secundaria. Lo que interesa son los signos esperados de los coeficientes de la regresión y su importancia práctica y/o estadística.\n",
    "\n",
    "\n",
    "Importamos los datos y revisamos la variable dependiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>GPA</th>\n",
       "      <th>LETTER</th>\n",
       "      <th>OBS</th>\n",
       "      <th>PSI</th>\n",
       "      <th>TUCE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRADE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      len                    \n",
       "      GPA LETTER OBS PSI TUCE\n",
       "GRADE                        \n",
       "0      21     21  21  21   21\n",
       "1      11     11  11  11   11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uu = \"https://raw.githubusercontent.com/vmoprojs/DataLectures/master/GA/tabla15_7.csv\"\n",
    "datos = pd.read_csv(uu,sep = ';')\n",
    "datos.columns\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "datos.pivot_table(index = 'GRADE', aggfunc = [len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.402801\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   No. Observations:                   32\n",
      "Model:                          Logit   Df Residuals:                       28\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 28 Mar 2023   Pseudo R-squ.:                  0.3740\n",
      "Time:                        04:00:30   Log-Likelihood:                -12.890\n",
      "converged:                       True   LL-Null:                       -20.592\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.001502\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -13.0213      4.931     -2.641      0.008     -22.687      -3.356\n",
      "GPA            2.8261      1.263      2.238      0.025       0.351       5.301\n",
      "TUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\n",
      "PSI            2.3787      1.065      2.234      0.025       0.292       4.465\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "ajuste1 = stm.logit('GRADE ~ GPA + TUCE + PSI',data = datos).fit()\n",
    "\n",
    "print(ajuste1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Marginal Effects</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th> <td>GRADE</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>        <td>dydx</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>At:</th>            <td>mean</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <th></th>      <th>dy/dx</th>    <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>  <td>    0.5339</td> <td>    0.237</td> <td>    2.252</td> <td> 0.024</td> <td>    0.069</td> <td>    0.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TUCE</th> <td>    0.0180</td> <td>    0.026</td> <td>    0.685</td> <td> 0.493</td> <td>   -0.033</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PSI</th>  <td>    0.4493</td> <td>    0.197</td> <td>    2.284</td> <td> 0.022</td> <td>    0.064</td> <td>    0.835</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "        Logit Marginal Effects       \n",
       "=====================================\n",
       "Dep. Variable:                  GRADE\n",
       "Method:                          dydx\n",
       "At:                              mean\n",
       "==============================================================================\n",
       "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "GPA            0.5339      0.237      2.252      0.024       0.069       0.998\n",
       "TUCE           0.0180      0.026      0.685      0.493      -0.033       0.069\n",
       "PSI            0.4493      0.197      2.284      0.022       0.064       0.835\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.get_margeff(at='mean').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07311607, 0.00246188, 0.06154047],\n",
       "       [0.15815168, 0.0053251 , 0.1331134 ],\n",
       "       [0.43011639, 0.01448239, 0.36202116],\n",
       "       [0.07130492, 0.0024009 , 0.06001605],\n",
       "       [0.69272252, 0.02332457, 0.58305197],\n",
       "       [0.09507939, 0.00320141, 0.08002659],\n",
       "       [0.0729182 , 0.00245522, 0.06137393],\n",
       "       [0.1381988 , 0.00465327, 0.11631942],\n",
       "       [0.2791564 , 0.00939944, 0.23496087],\n",
       "       [0.60069976, 0.02022608, 0.50559808]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = ajuste1.get_margeff(at='all').margeff\n",
    "mm[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Son, en conjunto, los coeficientes significativos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<Wald test (chi2): statistic=8.873128862019582, p-value=0.06435007959304277, df_denom=4>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp = '(Intercept = 0, GPA = 0,TUCE=0,PSI=0)'\n",
    "ajuste1.wald_test(hyp,scalar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept     0.000002\n",
       "GPA          16.879715\n",
       "TUCE          1.099832\n",
       "PSI          10.790732\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(ajuste1.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto indica que los estudiantes expuestos al nuevo método de enseñanza son 10 veces más propensos a obtener una A que quienes no están expuestos al nuevo método, en tanto no cambien los demás factores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02657799, 0.05950125, 0.18725993, 0.02590164, 0.56989295,\n",
       "       0.03485827, 0.02650406, 0.051559  , 0.11112666, 0.69351131,\n",
       "       0.02447037, 0.18999744, 0.32223955, 0.19321116, 0.36098992,\n",
       "       0.03018375, 0.05362641, 0.03858834, 0.58987249, 0.66078584,\n",
       "       0.06137585, 0.90484727, 0.24177245, 0.85209089, 0.83829051,\n",
       "       0.48113304, 0.63542059, 0.30721866, 0.84170413, 0.94534025,\n",
       "       0.5291172 , 0.11103084])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.predict(linear = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probit\n",
    "\n",
    "En los modelos logia se propuso la logística, en este caso se propone la Función de Distribución Acumulada Normal. Suponga que la variable de respuesta es binaria, 1 o 0. $Y$ podría representar la presencia/ausencia de una condición, éxito/fracaso, si/no. Se tiene también un vector de regresoras $X$, el modelo toma la forma:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\Pr(Y=1\\mid X)=\\Phi (X^{T}\\beta ),}\n",
    "$$\n",
    "\n",
    "donde $Pr$ es la prbabilidad y $\\Phi$ distribución acumulada de la normal estándar ${\\displaystyle \\Phi (x)={\\frac {1}{\\sqrt {2\\pi }}}\\int _{-\\infty }^{x}e^{-t^{2}/2}\\,dt}$. Los parámetros $\\beta$ se estiman típicamente con el método de máxima verosimilitud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.400588\n",
      "         Iterations 6\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   No. Observations:                   32\n",
      "Model:                         Probit   Df Residuals:                       28\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 28 Mar 2023   Pseudo R-squ.:                  0.3775\n",
      "Time:                        04:02:11   Log-Likelihood:                -12.819\n",
      "converged:                       True   LL-Null:                       -20.592\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.001405\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -7.4523      2.542     -2.931      0.003     -12.435      -2.469\n",
      "GPA            1.6258      0.694      2.343      0.019       0.266       2.986\n",
      "TUCE           0.0517      0.084      0.617      0.537      -0.113       0.216\n",
      "PSI            1.4263      0.595      2.397      0.017       0.260       2.593\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "ajuste1 = stm.probit('GRADE ~ GPA + TUCE + PSI',data = datos).fit()\n",
    "\n",
    "print(ajuste1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Marginal Effects</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th> <td>GRADE</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>        <td>dydx</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>At:</th>            <td>mean</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <th></th>      <th>dy/dx</th>    <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>  <td>    0.5333</td> <td>    0.232</td> <td>    2.294</td> <td> 0.022</td> <td>    0.078</td> <td>    0.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TUCE</th> <td>    0.0170</td> <td>    0.027</td> <td>    0.626</td> <td> 0.531</td> <td>   -0.036</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PSI</th>  <td>    0.4679</td> <td>    0.188</td> <td>    2.494</td> <td> 0.013</td> <td>    0.100</td> <td>    0.836</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "       Probit Marginal Effects       \n",
       "=====================================\n",
       "Dep. Variable:                  GRADE\n",
       "Method:                          dydx\n",
       "At:                              mean\n",
       "==============================================================================\n",
       "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "GPA            0.5333      0.232      2.294      0.022       0.078       0.989\n",
       "TUCE           0.0170      0.027      0.626      0.531      -0.036       0.070\n",
       "PSI            0.4679      0.188      2.494      0.013       0.100       0.836\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.get_margeff(at='mean').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07255307, 0.00230845, 0.06365122],\n",
       "       [0.17584323, 0.00559486, 0.15426826],\n",
       "       [0.44108314, 0.01403409, 0.38696473],\n",
       "       [0.07391114, 0.00235166, 0.06484266],\n",
       "       [0.64252589, 0.02044346, 0.5636916 ],\n",
       "       [0.10206861, 0.00324755, 0.08954537],\n",
       "       [0.07368267, 0.00234439, 0.06464222],\n",
       "       [0.15292232, 0.00486558, 0.13415961],\n",
       "       [0.3033272 , 0.00965106, 0.26611067],\n",
       "       [0.59359928, 0.01888675, 0.520768  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = ajuste1.get_margeff(at='all').margeff\n",
    "mm[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01817074, 0.05308048, 0.18992627, 0.01857067, 0.55457485,\n",
       "       0.02723306, 0.01850327, 0.04457139, 0.10880811, 0.66312068,\n",
       "       0.01610236, 0.19355656, 0.32332821, 0.19518259, 0.35634057,\n",
       "       0.02196542, 0.04569425, 0.03085127, 0.59340229, 0.65718629,\n",
       "       0.06192878, 0.90453884, 0.2731908 , 0.84745013, 0.83419472,\n",
       "       0.48872605, 0.64240727, 0.3286732 , 0.84001686, 0.95224465,\n",
       "       0.53995949, 0.123544  ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajuste1.predict(linear = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
