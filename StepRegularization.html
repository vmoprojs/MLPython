

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>10. Selección y regularización de modelos lineales &#8212; Técnicas de Machine Learning con Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'StepRegularization';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="11. Árboles de decisión" href="Arboles.html" />
    <link rel="prev" title="9. Modelos de Probabilidad" href="ProbModels.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Machine Learning con Python
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introducción a Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="IntroduccionPython.html">1. Python, una introducción</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pandas.html">2. Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="Visualizacion.html">3. Gráficos básicos: matplotlib &amp; seaborn</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Análisis Exploratorio de Datos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Descriptiva.html">4. Estadística Descriptiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="PruebasHipotesis.html">5. Pruebas de Hipótesis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Aprendizaje Supervizado</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="IntroML.html">6. Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="AprendizajeSupervisado.html">7. Aprendizaje Supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="Regresion.html">8. Regresión lineal</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProbModels.html">9. Modelos de Probabilidad</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Selección y regularización de modelos lineales</a></li>
<li class="toctree-l1"><a class="reference internal" href="Arboles.html">11. Árboles de decisión</a></li>
<li class="toctree-l1"><a class="reference internal" href="SVM.html">12. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="RedesNeuronales.html">13. Redes Neuronales</a></li>
<li class="toctree-l1"><a class="reference internal" href="Evaluacion.html">14. Evaluación de Modelos</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Aprendizaje no Supervisado</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ACP.html">15. Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="NoSupervizado.html">16. Aprendizaje no supervizado</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/vmoprojs/MLPython" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/vmoprojs/MLPython/issues/new?title=Issue%20on%20page%20%2FStepRegularization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/StepRegularization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Selección y regularización de modelos lineales</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-variables">10.1. Selección de variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-las-mejores-variables">10.1.1. Selección de las mejores variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-adelante">10.1.2. Selección <em>stepwise</em> hacia adelante</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-atras">10.1.3. Selección <em>stepwise</em> hacia atrás</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo">10.1.3.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizacion">10.2. Regularización</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge">10.2.1. Regresión <em>Ridge</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lasso">10.2.2. Regresión <em>Lasso</em></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">10.2.2.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="seleccion-y-regularizacion-de-modelos-lineales">
<h1><span class="section-number">10. </span>Selección y regularización de modelos lineales<a class="headerlink" href="#seleccion-y-regularizacion-de-modelos-lineales" title="Permalink to this heading">#</a></h1>
<p>A pesar de su simplicidad, los modelos lineales tienen ventajas de interpretabilidad y con frecuencia muestran buen rendimiento en predicción</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_0+\beta_1X_1+\cdots+\beta_pX_p+\epsilon.
\]</div>
<p>Esta sección aborda el cambiar el ajuste de mínimos cuadrados ordinarios (OLS) tradicional por métodos alternativos que se apalancan en OLS.</p>
<p><strong>¿Por que usar alternativas a OLS?</strong></p>
<ul class="simple">
<li><p>Predictibilidad: Si la relación entre las variables es aproximadamente lineal y si <span class="math notranslate nohighlight">\(n\gg p\)</span> (el número de observaciones es más grande que el número de variables), entonces OLS tiene poco sesgo y rinde bien en datos <em>test</em>. Sin embargo, si <span class="math notranslate nohighlight">\(n\)</span> no es tan grande respecto a <span class="math notranslate nohighlight">\(p\)</span>, puede haber mucha variabilidad en OLS, lo que resulta en sobreajuste y mal rendimiento en <em>test</em>.</p></li>
</ul>
<p>Algunos de los problemas que surgen cuando <span class="math notranslate nohighlight">\(n\approx p\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Sobreajuste (Overfitting)</strong></p></li>
</ol>
<p>El <strong>error cuadrático medio</strong> (MSE) en el conjunto de entrenamiento se define como:</p>
<div class="math notranslate nohighlight">
\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> es el valor observado,</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> es la predicción del modelo,</p></li>
<li><p><span class="math notranslate nohighlight">\( n \)</span> es el número de observaciones.</p></li>
</ul>
<p>Cuando el número de variables <span class="math notranslate nohighlight">\(p\)</span> se aproxima a <span class="math notranslate nohighlight">\(n\)</span>, el modelo puede ajustarse casi perfectamente, lo que minimiza el MSE en el conjunto de entrenamiento, pero aumenta el error en nuevos datos (error de generalización). Este es el problema del sobreajuste.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Multicolinealidad</strong></p></li>
</ol>
<p>El <strong>problema de multicolinealidad</strong> se detecta cuando las variables explicativas están altamente correlacionadas entre sí. Un indicador común es el <strong>Factor de Inflación de la Varianza (VIF)</strong> para cada variable <span class="math notranslate nohighlight">\(j\)</span>, que se define como:</p>
<div class="math notranslate nohighlight">
\[
VIF_j = \frac{1}{1 - R_j^2}
\]</div>
<p>donde <span class="math notranslate nohighlight">\( R_j^2 \)</span> es el coeficiente de determinación de la regresión de la variable <span class="math notranslate nohighlight">\( X_j \)</span> sobre todas las demás variables.</p>
<p>Cuando <span class="math notranslate nohighlight">\( VIF_j \)</span> es grande (mayor a 10, por ejemplo), indica una alta colinealidad. Esto causa inestabilidad en las estimaciones de los coeficientes <span class="math notranslate nohighlight">\( \hat{\beta}_j \)</span>, lo que puede hacer que el modelo sea sensible a pequeños cambios en los datos.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Modelo No Identificable</strong></p></li>
</ol>
<p>En regresión lineal, los coeficientes de los parámetros <span class="math notranslate nohighlight">\( \boldsymbol{\beta} = (\beta_1, \beta_2, \dots, \beta_p) \)</span> se obtienen resolviendo el sistema de ecuaciones lineales:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]</div>
<p>donde:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{y} \)</span> es el vector de observaciones (de dimensión <span class="math notranslate nohighlight">\( n \times 1 \)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{X} \)</span> es la matriz de diseño (de dimensión <span class="math notranslate nohighlight">\( n \times p \)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span> es el vector de coeficientes,</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{\epsilon} \)</span> es el término de error.</p></li>
</ul>
<p>Para resolver <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span>, necesitamos invertir la matriz <span class="math notranslate nohighlight">\( \mathbf{X}^T \mathbf{X} \)</span>. Si <span class="math notranslate nohighlight">\( p \geq n \)</span>, la matriz <span class="math notranslate nohighlight">\( \mathbf{X}^T \mathbf{X} \)</span> no es invertible, lo que significa que el sistema es <strong>no identificable</strong> y no existe una única solución para <span class="math notranslate nohighlight">\( \boldsymbol{\beta} \)</span>.</p>
<div class="math notranslate nohighlight">
\[
\text{Si } \det(\mathbf{X}^T \mathbf{X}) = 0, \text{ entonces no hay solución única.}
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Varianza Alta</strong></p></li>
</ol>
<p>Cuando hay más variables que observaciones, el <strong>error estándar</strong> de los coeficientes de regresión se incrementa. La varianza de los coeficientes <span class="math notranslate nohighlight">\( \hat{\beta}_j \)</span> está dada por:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(\hat{\beta}_j) = \sigma^2 \left( (\mathbf{X}^T \mathbf{X})^{-1} \right)_{jj}
\]</div>
<p>donde <span class="math notranslate nohighlight">\( \sigma^2 \)</span> es la varianza del error residual. Si <span class="math notranslate nohighlight">\( \mathbf{X}^T \mathbf{X} \)</span> es mal condicionada (debido a la colinealidad o alta dimensionalidad), los elementos de la matriz inversa <span class="math notranslate nohighlight">\( (\mathbf{X}^T \mathbf{X})^{-1} \)</span> pueden ser muy grandes, lo que resulta en una varianza elevada para <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>. Esto significa que los coeficientes son altamente sensibles a los cambios en los datos.</p>
<ul class="simple">
<li><p>Interpretabilidad: Incluir variables independientes sin asociación con la variable dependiente en un modelo de regresión resulta en incluir complejidades innecesarias en el modelo. Si son removidas (se fijan sus coeficientes en cero), se puede obtener un modelo que es más interpretable. En OLS hay muy poca probabilidad de que se tengan coeficientes iguales a cero. Al eliminar variables estamos haciendo selección de variables o <em>feature selection</em>: selección de variables, contracción o regularización y reducción de dimensionalidad.</p></li>
</ul>
<section id="seleccion-de-variables">
<h2><span class="section-number">10.1. </span>Selección de variables<a class="headerlink" href="#seleccion-de-variables" title="Permalink to this heading">#</a></h2>
<section id="seleccion-de-las-mejores-variables">
<h3><span class="section-number">10.1.1. </span>Selección de las mejores variables<a class="headerlink" href="#seleccion-de-las-mejores-variables" title="Permalink to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Algoritmo: Selección de las mejores variables</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. Sea <span class="math notranslate nohighlight">\(\mathcal{M_0}\)</span> el modelo nulo (sin predictores). Este modelo solo predice la media muestral para cada observación.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. Para <span class="math notranslate nohighlight">\(k = 1,2,\ldots,p\)</span>: <br/></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code> </code> a. Ajusta todos <span class="math notranslate nohighlight">\({p \choose k}\)</span> modelos que contienen exactamente <span class="math notranslate nohighlight">\(k\)</span> predictores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code> </code> b. Selecciona el mejor de los <span class="math notranslate nohighlight">\({p \choose k}\)</span> modelos, nómbralo <span class="math notranslate nohighlight">\(\mathcal{M_k}\)</span>. El <em>mejor</em> modelo se define como aquel que tiene el mejor <span class="math notranslate nohighlight">\(RRS\)</span> (suma de residuos al cuadrado), o de manera equivalente el más alto <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3. Selecciona el mejor modelo de los <span class="math notranslate nohighlight">\(\mathcal{M_0},\ldots,\mathcal{M_k}\)</span> usando el error de predicción en un conjunto de validación, <span class="math notranslate nohighlight">\(C_p\)</span>, AIC, BIC o <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. O usa el método de validación cruzada.</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Nota que este algoritmo no puede ser aplicado cuando <span class="math notranslate nohighlight">\(p\)</span> es grande.</p></li>
<li><p>También puede tener problemas de estadísticos cuando <span class="math notranslate nohighlight">\(p\)</span> es grande. Cuanto mayor sea el espacio de búsqueda, mayor será la posibilidad de encontrar modelos que <strong>se vean bien en los datos de entrenamiento</strong>, aunque podrían no tener <strong>ningún poder predictivo</strong> sobre datos futuros.</p></li>
<li><p>Por lo tanto, un espacio de búsqueda enorme puede dar lugar a un sobreajuste y a una alta varianza de las estimaciones de los coeficientes.</p></li>
<li><p>Por los motivos antes listado, los métodos <em>stepwise</em> o <em>paso a paso</em> son más atractivos.</p></li>
</ul>
<p><strong><span class="math notranslate nohighlight">\(C_p\)</span> de Mallow</strong></p>
<div class="math notranslate nohighlight">
\[
C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)
\]</div>
<p>donde <span class="math notranslate nohighlight">\(d\)</span> es el total de parámetros usados y <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> es la estimación de la varianza del error <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p><strong>Criterio AIC</strong></p>
<p>Se puede usar en modelos donde el ajuste se realiza con máxima verosimilitud:</p>
<div class="math notranslate nohighlight">
\[
AIC = -2\text{log}L+2d
\]</div>
<p>Donde <span class="math notranslate nohighlight">\(L\)</span> es el valor máximo de la función de máxima verosimilitud del modelo.</p>
<div class="math notranslate nohighlight">
\[
BIC =  \frac{1}{n}(RSS+\text{log}(n)d\hat{\sigma}^2)
\]</div>
<p>donde <span class="math notranslate nohighlight">\(n\)</span> es el número de observaciones.</p>
<p><strong><span class="math notranslate nohighlight">\(R^2\)</span> ajustado</strong></p>
<div class="math notranslate nohighlight">
\[
R^2_{adj} = 1-\frac{RSS/(n-d-1)}{TSS/(n-1)}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(TSS\)</span> es la suma total de cuadrados.</p>
</section>
<section id="seleccion-stepwise-hacia-adelante">
<h3><span class="section-number">10.1.2. </span>Selección <em>stepwise</em> hacia adelante<a class="headerlink" href="#seleccion-stepwise-hacia-adelante" title="Permalink to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Algoritmo: Selección <em>stepwise</em> hacia adelante</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. Sea <span class="math notranslate nohighlight">\(\mathcal{M_0}\)</span> el modelo nulo (sin predictores). Este modelo solo predice la media muestral para cada observación.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. Para <span class="math notranslate nohighlight">\(k = 0,2,\ldots,p-1\)</span>: <br/></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code> </code> a. Considera todos los <span class="math notranslate nohighlight">\(p-k\)</span> modelos que aumentan predictores en <span class="math notranslate nohighlight">\(\mathcal{M_k}\)</span> con un predictor adicional.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code> </code> b. Elige el mejor de estos <span class="math notranslate nohighlight">\(p-k\)</span> modelos, y nómbralo <span class="math notranslate nohighlight">\(\mathcal{M_{k+1}}\)</span>. El <em>mejor</em> modelo se define como aquel que tiene el mejor <span class="math notranslate nohighlight">\(RRS\)</span>, o el más alto <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3. Selecciona el mejor modelo de los <span class="math notranslate nohighlight">\(\mathcal{M_0},\ldots,\mathcal{M_k}\)</span> usando el error de predicción en un conjunto de validación, <span class="math notranslate nohighlight">\(C_p\)</span>, AIC, BIC o <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. O usa el método de validación cruzada.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="seleccion-stepwise-hacia-atras">
<h3><span class="section-number">10.1.3. </span>Selección <em>stepwise</em> hacia atrás<a class="headerlink" href="#seleccion-stepwise-hacia-atras" title="Permalink to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Algoritmo: Selección <em>stepwise</em> hacia atrás</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. Sea <span class="math notranslate nohighlight">\(\mathcal{M_p}\)</span> el modelo completo, que contiene todos los <span class="math notranslate nohighlight">\(p\)</span> predictores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. Para <span class="math notranslate nohighlight">\(k = p,p-1,\ldots,1\)</span>: <br/></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code> </code> a. Considera todos los <span class="math notranslate nohighlight">\(k\)</span> modelos que contienen todos menos uno de los predictores en en <span class="math notranslate nohighlight">\(\mathcal{M_k}\)</span>, para un total de <span class="math notranslate nohighlight">\(k-1\)</span> predictores.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code> </code> b. Elige el mejor de estos <span class="math notranslate nohighlight">\(k\)</span> modelos, y nómbralo <span class="math notranslate nohighlight">\(\mathcal{M_{k-1}}\)</span>. El <em>mejor</em> modelo se define como aquel que tiene el mejor <span class="math notranslate nohighlight">\(RRS\)</span>, o el más alto <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>3. Selecciona el mejor modelo de los <span class="math notranslate nohighlight">\(\mathcal{M_0},\ldots,\mathcal{M_k}\)</span> usando el error de predicción en un conjunto de validación, <span class="math notranslate nohighlight">\(C_p\)</span>, AIC, BIC o <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. O usa el método de validación cruzada.</p></td>
</tr>
</tbody>
</table>
<section id="ejemplo">
<h4><span class="section-number">10.1.3.1. </span>Ejemplo<a class="headerlink" href="#ejemplo" title="Permalink to this heading">#</a></h4>
<p>La superintendencia de compañias del Ecuador tiene información de estados financieros de las empresas que regula. Vamos a predecir los ingresos por ventas de las empresas grandes del 2023. En la pestaña <em>Recursos</em> del siguiente enlace: https://appscvsmovil.supercias.gob.ec/ranking/reporte.html</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">uu</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/vmoprojs/DataLectures/refs/heads/master/superciasGrandes2023.csv&quot;</span>
<span class="n">datos</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">uu</span><span class="p">)</span>
<span class="n">datos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">],</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Supongamos que &#39;X&#39; es tu conjunto de características y &#39;y&#39; la variable dependiente.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="o">!=</span><span class="s1">&#39;ingresos_ventas&#39;</span><span class="p">]</span>  <span class="c1"># tus datos de características</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">datos</span><span class="p">[</span><span class="s1">&#39;ingresos_ventas&#39;</span><span class="p">]</span>  <span class="c1"># tu variable dependiente</span>


<span class="n">start_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">start_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:        ingresos_ventas   R-squared (uncentered):                   1.000
Model:                            OLS   Adj. R-squared (uncentered):              1.000
Method:                 Least Squares   F-statistic:                          3.988e+05
Date:                Tue, 24 Sep 2024   Prob (F-statistic):                        0.00
Time:                        05:59:45   Log-Likelihood:                         -46254.
No. Observations:                2969   AIC:                                  9.260e+04
Df Residuals:                    2925   BIC:                                  9.286e+04
Df Model:                          44                                                  
Covariance Type:            nonrobust                                                  
=============================================================================================
                                coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------
activos                       0.0022      0.002      1.338      0.181      -0.001       0.005
patrimonio                    0.0018      0.002      1.005      0.315      -0.002       0.005
utilidad_an_imp               0.0085      0.005      1.864      0.062      -0.000       0.018
impuesto_renta                0.1804      0.031      5.773      0.000       0.119       0.242
n_empleados                 -80.0757     60.249     -1.329      0.184    -198.211      38.060
utilidad_ejercicio            0.9299      0.009    108.935      0.000       0.913       0.947
utilidad_neta                 0.0373      0.014      2.612      0.009       0.009       0.065
liquidez_corriente        -2.379e+04    3.4e+04     -0.699      0.485   -9.05e+04    4.29e+04
prueba_acida               2.456e+04    3.4e+04      0.722      0.470   -4.22e+04    9.13e+04
end_activo                 3.941e+04   4.04e+04      0.976      0.329   -3.98e+04    1.19e+05
end_patrimonial            2.493e+04   8.14e+04      0.306      0.759   -1.35e+05    1.85e+05
end_activo_fijo              14.0003     12.575      1.113      0.266     -10.656      38.656
end_corto_plazo           -6.517e+04   1.08e+05     -0.605      0.546   -2.77e+05    1.46e+05
end_largo_plazo           -2.653e+05   1.45e+05     -1.824      0.068   -5.51e+05    1.99e+04
cobertura_interes             0.2875      0.243      1.184      0.236      -0.188       0.763
apalancamiento            -2.354e+04   8.17e+04     -0.288      0.773   -1.84e+05    1.37e+05
apalancamiento_financiero    20.3885    147.134      0.139      0.890    -268.109     308.886
end_patrimonial_ct         1.146e+07   9.41e+06      1.217      0.224      -7e+06    2.99e+07
end_patrimonial_nct        1.146e+07   9.41e+06      1.218      0.223   -6.99e+06    2.99e+07
apalancamiento_c_l_plazo  -1.146e+07   9.41e+06     -1.217      0.224   -2.99e+07    6.99e+06
rot_cartera                   0.5832      0.595      0.980      0.327      -0.584       1.751
rot_activo_fijo              -7.2259      6.715     -1.076      0.282     -20.393       5.941
rot_ventas                  840.1566   4442.722      0.189      0.850   -7871.024    9551.337
per_med_cobranza             -0.0365      0.019     -1.880      0.060      -0.075       0.002
per_med_pago                  0.0002      0.000      0.438      0.661      -0.001       0.001
impac_gasto_a_v           -5.696e+04   2.26e+04     -2.525      0.012   -1.01e+05   -1.27e+04
impac_carga_finan         -1.448e+06   6.97e+05     -2.077      0.038   -2.81e+06   -8.12e+04
rent_neta_activo           2.565e+05   1.75e+06      0.147      0.883   -3.17e+06    3.69e+06
margen_bruto                4.04e+05   1.09e+05      3.700      0.000     1.9e+05    6.18e+05
margen_operacional           -0.0014      0.000     -2.836      0.005      -0.002      -0.000
rent_neta_ventas          -1.287e+06   2.43e+05     -5.298      0.000   -1.76e+06   -8.11e+05
rent_ope_patrimonio          -0.8571    604.515     -0.001      0.999   -1186.175    1184.461
rent_ope_activo             191.7623   6593.384      0.029      0.977   -1.27e+04    1.31e+04
roe                         313.5004   2139.718      0.147      0.884   -3882.007    4509.008
roa                       -4157.0503   1.74e+06     -0.002      0.998   -3.41e+06     3.4e+06
fortaleza_patrimonial     -2.554e+04   2.32e+04     -1.102      0.271    -7.1e+04    1.99e+04
gastos_financieros            0.1684      0.018      9.564      0.000       0.134       0.203
gastos_admin_ventas           0.1687      0.011     15.215      0.000       0.147       0.190
depreciaciones                0.0767      0.015      4.997      0.000       0.047       0.107
amortizaciones               -0.0022      0.020     -0.114      0.909      -0.041       0.036
costos_ventas_prod            0.9988      0.001   1677.114      0.000       0.998       1.000
deuda_total                  -0.0084      0.005     -1.838      0.066      -0.017       0.001
deuda_total_c_plazo          -0.0514      0.015     -3.418      0.001      -0.081      -0.022
total_gastos                  0.8091      0.011     73.615      0.000       0.788       0.831
==============================================================================
Omnibus:                     4441.075   Durbin-Watson:                   1.975
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2756805.516
Skew:                          -8.879   Prob(JB):                         0.00
Kurtosis:                     151.221   Cond. No.                     9.38e+10
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 9.38e+10. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Modelo hacia adelante:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="c1"># Crear el metodo para modelo de regresión lineal</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Crear un selector de características secuencial</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;forward&#39;</span><span class="p">)</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Ver las características seleccionadas</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>

<span class="c1"># Ajustar el modelo final con las características seleccionadas</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_selected</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Mostrar el resumen del modelo final</span>
<span class="nb">print</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:        ingresos_ventas   R-squared (uncentered):                   1.000
Model:                            OLS   Adj. R-squared (uncentered):              1.000
Method:                 Least Squares   F-statistic:                          7.077e+05
Date:                Tue, 24 Sep 2024   Prob (F-statistic):                        0.00
Time:                        05:59:53   Log-Likelihood:                         -46443.
No. Observations:                2969   AIC:                                  9.293e+04
Df Residuals:                    2947   BIC:                                  9.306e+04
Df Model:                          22                                                  
Covariance Type:            nonrobust                                                  
=============================================================================================
                                coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------
patrimonio                    0.0029      0.001      3.844      0.000       0.001       0.004
impuesto_renta                0.2647      0.028      9.550      0.000       0.210       0.319
utilidad_ejercicio            0.9515      0.005    173.593      0.000       0.941       0.962
liquidez_corriente        -3.969e+04   3.34e+04     -1.187      0.235   -1.05e+05    2.59e+04
prueba_acida               3.978e+04   3.34e+04      1.190      0.234   -2.58e+04    1.05e+05
end_activo                  2.92e+04   3.59e+04      0.814      0.416   -4.12e+04    9.96e+04
end_patrimonial             947.0902   1750.644      0.541      0.589   -2485.518    4379.699
end_corto_plazo           -1.249e+05   5.53e+04     -2.258      0.024   -2.33e+05   -1.65e+04
cobertura_interes             0.1899      0.251      0.757      0.449      -0.302       0.682
apalancamiento_financiero    -3.6231    155.722     -0.023      0.981    -308.957     301.711
end_patrimonial_nct         476.8089   1.85e+04      0.026      0.979   -3.57e+04    3.67e+04
rot_cartera                   0.4658      0.625      0.746      0.456      -0.759       1.691
rot_activo_fijo               0.2106      0.991      0.212      0.832      -1.733       2.154
per_med_cobranza             -0.0552      0.019     -2.857      0.004      -0.093      -0.017
per_med_pago                  0.0003      0.000      0.658      0.511      -0.001       0.001
impac_gasto_a_v            -6.51e+04   2.33e+04     -2.791      0.005   -1.11e+05   -1.94e+04
margen_operacional           -0.0018      0.000     -4.198      0.000      -0.003      -0.001
roe                         812.0868   1069.241      0.759      0.448   -1284.449    2908.622
fortaleza_patrimonial     -2.909e+04   2.43e+04     -1.198      0.231   -7.67e+04    1.85e+04
costos_ventas_prod            0.9975      0.001   1984.810      0.000       0.997       0.999
deuda_total                  -0.0100      0.004     -2.244      0.025      -0.019      -0.001
total_gastos                  0.9822      0.002    614.088      0.000       0.979       0.985
==============================================================================
Omnibus:                     5205.368   Durbin-Watson:                   1.973
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          6226013.432
Skew:                         -12.165   Prob(JB):                         0.00
Kurtosis:                     226.016   Cond. No.                     2.29e+08
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 2.29e+08. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Modelo hacia atrás</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="c1"># Crear el metodo para modelo de regresión lineal</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Crear un selector de características secuencial</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;backward&#39;</span><span class="p">)</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Ver las características seleccionadas</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>

<span class="c1"># Ajustar el modelo final con las características seleccionadas</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">selected_features</span><span class="p">]</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_selected</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Mostrar el resumen del modelo final</span>
<span class="nb">print</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:        ingresos_ventas   R-squared (uncentered):                   1.000
Model:                            OLS   Adj. R-squared (uncentered):              1.000
Method:                 Least Squares   F-statistic:                          7.069e+05
Date:                Tue, 24 Sep 2024   Prob (F-statistic):                        0.00
Time:                        06:00:07   Log-Likelihood:                         -46444.
No. Observations:                2969   AIC:                                  9.293e+04
Df Residuals:                    2947   BIC:                                  9.306e+04
Df Model:                          22                                                  
Covariance Type:            nonrobust                                                  
=============================================================================================
                                coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------
patrimonio                    0.0029      0.001      3.825      0.000       0.001       0.004
impuesto_renta                0.2655      0.028      9.574      0.000       0.211       0.320
utilidad_ejercicio            0.9523      0.005    173.243      0.000       0.942       0.963
liquidez_corriente        -2.521e+04   3.46e+04     -0.728      0.466   -9.31e+04    4.27e+04
prueba_acida               2.537e+04   3.46e+04      0.733      0.464   -4.25e+04    9.32e+04
end_patrimonial            1.049e+05   6.34e+04      1.655      0.098   -1.94e+04    2.29e+05
end_corto_plazo            4316.3368   8.71e+04      0.050      0.960   -1.66e+05    1.75e+05
cobertura_interes             0.1918      0.251      0.763      0.446      -0.301       0.685
apalancamiento            -1.032e+05   6.35e+04     -1.627      0.104   -2.28e+05    2.12e+04
apalancamiento_financiero     0.5842    155.917      0.004      0.997    -305.133     306.301
end_patrimonial_nct         442.1157   1.84e+04      0.024      0.981   -3.55e+04    3.64e+04
rot_cartera                   0.4922      0.626      0.786      0.432      -0.736       1.720
per_med_cobranza             -0.0577      0.019     -2.994      0.003      -0.096      -0.020
per_med_pago                  0.0003      0.000      0.684      0.494      -0.001       0.001
impac_gasto_a_v           -6.215e+04   2.33e+04     -2.672      0.008   -1.08e+05   -1.65e+04
rent_neta_activo          -2.551e+05   1.85e+06     -0.138      0.891   -3.89e+06    3.38e+06
margen_operacional           -0.0016      0.000     -3.870      0.000      -0.002      -0.001
rent_ope_patrimonio         132.6884    233.531      0.568      0.570    -325.211     590.588
roa                        2.103e+05   1.84e+06      0.114      0.909    -3.4e+06    3.82e+06
fortaleza_patrimonial     -2.853e+04   2.42e+04     -1.178      0.239    -7.6e+04     1.9e+04
costos_ventas_prod            0.9970      0.000   2223.541      0.000       0.996       0.998
total_gastos                  0.9818      0.002    626.940      0.000       0.979       0.985
==============================================================================
Omnibus:                     5206.591   Durbin-Watson:                   1.971
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          6195771.326
Skew:                         -12.174   Prob(JB):                         0.00
Kurtosis:                     225.465   Cond. No.                     9.63e+09
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 9.63e+09. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="regularizacion">
<h2><span class="section-number">10.2. </span>Regularización<a class="headerlink" href="#regularizacion" title="Permalink to this heading">#</a></h2>
<p>La regularización es un método que nos permite restringir el proceso de estimación, se usa para evitar un posible sobre ajuste del modelo (<em>overfitting</em>).</p>
<p>Una forma de hacer regularización es donde los coeficientes de variables en el modelo no sean muy grandes (regresión <em>ridge</em>). Otra forma es contraer los coeficientes, llegando a tener coeficientes iguales a cero (regresión <em>lasso</em>).</p>
<p>Como regla general, el segundo enfoque suele ser mejor que el primero.</p>
<p>Los coeficientes regularizados se obtiene usando una función de penalidad <span class="math notranslate nohighlight">\(p(\boldsymbol{\alpha})\)</span> para restringir el tamaño del vector de coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = (\alpha_1,\cdots,\alpha_M)^T\)</span> del predictor <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \sum_{j = 1}^{M} \alpha_jC_j(\mathbf{x})\)</span>. Los coeficientes penalizados son obtenidos como solución al problema de minimización:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\alpha}}_{\lambda}= \underset{\boldsymbol{\alpha}}{\mathrm{argmin}} = \left\{\sum_{i = 1}^{n}L(y_i,f(\mathbf{x}_i))+\lambda p(\boldsymbol{\alpha}) \right\},
\]</div>
<p>donde <span class="math notranslate nohighlight">\(L\)</span> es una función de pérdida y <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> es un parámetro de regularización también conocido como <em>ratio de aprendizaje</em>.</p>
<p>Existen diferentes opciones para funciones de pérdida:</p>
<ul class="simple">
<li><p>Exponencial:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f(\mathbf{x})) = e^{-yf(\mathbf{x})}, y \in \{-1,+1\}.
\]</div>
<ul class="simple">
<li><p>Logística:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f_t(\mathbf{x})) = \text{log}\{1+ e^{-2yf_t(\mathbf{x})}\},y \in \{-1,+1\}.
\]</div>
<ul class="simple">
<li><p>Error cuadrático:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f_t(\mathbf{x})) = \frac{1}{2}(y-f_t(\mathbf{x}))^2, y \in \mathcal{R}.
\]</div>
<ul class="simple">
<li><p>Error absoluto:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(y,f_t(\mathbf{x})) =  |y-f_t(\mathbf{x})|, y \in \mathcal{R}.
\]</div>
<ul class="simple">
<li><p>Huber</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
L(y,f_t(\mathbf{x})) = \begin{cases}
\frac{1}{2}(y-f_t(\mathbf{x}))^2, &amp; \text{if } |y-f_t(\mathbf{x})|\leq\delta,\\
\delta(|y-f_t(\mathbf{x})|-\delta/2), &amp; \text{en otro caso.}
\end{cases}
\end{split}\]</div>
<a class="reference internal image-reference" href="_images/L1_fig1.png"><img alt="_images/L1_fig1.png" src="_images/L1_fig1.png" style="width: 500px; height: 300px;" /></a>
<p>Hay dos tipos de funciones de penalidad:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_2\)</span>: esta función de penalidad restringe la suma de cuadrados de los coeficientes,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_2(\boldsymbol{\alpha})=\sum_{j = 1}^{M} \alpha_j^2.
\]</div>
<p>Cuando <span class="math notranslate nohighlight">\(L\)</span> combinado es una combinación convexa y usamos la pérdida de error cuadrático, el predictor de regresión penalizado óptimo es el estimador de regresión <em>ridge</em>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L_1\)</span>: Los coeficientes se restringen tal que su suma de valores absolutos,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_1(\boldsymbol{\alpha})=\sum_{j = 1}^{M} |\alpha_j|.
\]</div>
<p>sea menor que un valor dado. La evidencia empírica sugiere que la penalización <span class="math notranslate nohighlight">\(L_1\)</span> (<em>lasso</em>) funciona mejor cuando hay un número pequeño o mediano de coeficientes verdaderos de tamaño moderado.</p>
<section id="regresion-ridge">
<h3><span class="section-number">10.2.1. </span>Regresión <em>Ridge</em><a class="headerlink" href="#regresion-ridge" title="Permalink to this heading">#</a></h3>
<p>En mínimos cuadrados ordinarios, las estimaciones de <span class="math notranslate nohighlight">\(\beta_0,\beta_1,\ldots,\beta_p\)</span> se obtienen minimizando</p>
<div class="math notranslate nohighlight">
\[
\text{RSS} = \sum_{i = 1}^n\left(y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right)^2.
\]</div>
<p>La <strong>regresión <em>ridge</em></strong> es muy similar al enfoque de mínimos cuadrados, pero los coeficientes de la regresión <em>ridge</em> <span class="math notranslate nohighlight">\(\hat{\beta}^R\)</span> son los valores que minimizan</p>
<div class="math notranslate nohighlight">
\[
\sum_{i = 1}^n\left(y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right)^2+\lambda\sum_{j = 1}^{p} \beta_j^2 = \text{RSS}+\lambda\sum_{j = 1}^{p} \beta_j^2,
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\lambda\)</span> es un parámetro ajustable que se determina de manera separada. La influencia de la regularización se controla con <span class="math notranslate nohighlight">\(\lambda\)</span>. Valores altos de  <span class="math notranslate nohighlight">\(\lambda\)</span> significa más regularización y modelos más simples. Sin embargo, la regresión <code class="docutils literal notranslate"><span class="pre">ridge</span></code> siempre generará un modelo que incluya todos los predictores. Incrementar el valor de <span class="math notranslate nohighlight">\(\lambda\)</span> tenderá a reducir las magnitudes de los coeficientes, pero no resultará en la exclusión de ninguna de las variables. Notemos que la penalidad no se aplica a <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</section>
<section id="regresion-lasso">
<h3><span class="section-number">10.2.2. </span>Regresión <em>Lasso</em><a class="headerlink" href="#regresion-lasso" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\sum_{i = 1}^n\left(y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right)^2+\lambda\sum_{j = 1}^{p} |\beta_j| = \text{RSS}+\lambda\sum_{j = 1}^{p} |\beta_j|,
\]</div>
<p>La penalización <span class="math notranslate nohighlight">\(L_1\)</span> tiene el efecto de forzar algunas de las estimaciones de coeficientes a ser <strong>exactamente iguales a cero</strong> cuando el parámetro <span class="math notranslate nohighlight">\(\lambda\)</span> de ajuste es suficientemente grande. Por lo tanto, <em>lasso</em> realiza <strong>selección de variables</strong>. Como resultado, los modelos generados a partir de <em>lasso</em> son generalmente mucho más <strong>fáciles de interpretar</strong> que los producidos por regresión <em>ridge</em>. Decimos que <em>lasso</em> produce modelos dispersos (<em>sparse</em>), es decir, modelos que involucran solo un subconjunto de variables predictoras.</p>
<section id="id1">
<h4><span class="section-number">10.2.2.1. </span>Ejemplo<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>


<span class="c1"># Dividir los datos en entrenamiento y prueba</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>


<span class="c1"># Escalar los datos</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Crear modelos de Ridge y Lasso</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># Alpha controla la penalización</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1000.0</span><span class="p">)</span>
<span class="n">regOLS</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Entrenar los modelos</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">regOLS</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predicciones</span>
<span class="n">y_pred_ridge</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_lasso</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">regOLS</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># Calcular el error cuadrático medio (MSE)</span>
<span class="n">mse_ridge</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">)</span><span class="o">/</span><span class="mi">1000000</span>
<span class="n">mse_lasso</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lasso</span><span class="p">)</span><span class="o">/</span><span class="mi">1000000</span>
<span class="n">mse_ols</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ols</span><span class="p">)</span><span class="o">/</span><span class="mi">1000000</span>

<span class="c1"># Resultados</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Ridge: </span><span class="si">{</span><span class="n">mse_ridge</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Lasso: </span><span class="si">{</span><span class="n">mse_lasso</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE OLS: </span><span class="si">{</span><span class="n">mse_ols</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE Ridge: 1424589.67
MSE Lasso: 1400394.87
MSE OLS: 1392299.62
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Mostrar coeficientes con nombres</span>
<span class="n">ridge_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Ridge Coefficients&#39;</span><span class="p">])</span>
<span class="n">lasso_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Lasso Coefficients&#39;</span><span class="p">])</span>
<span class="n">ols_coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">regOLS</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;OLS Coefficients&#39;</span><span class="p">])</span>

<span class="c1"># Combinar coeficientes en un solo DataFrame</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ridge_coefficients</span><span class="p">,</span> <span class="n">lasso_coefficients</span><span class="p">,</span><span class="n">ols_coefficients</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Coeficientes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coeficientes:
                           Ridge Coefficients  Lasso Coefficients  \
activos                          2.582039e+05        0.000000e+00   
patrimonio                       2.274355e+05        3.544481e+05   
utilidad_an_imp                  1.539821e+05        1.839588e+05   
impuesto_renta                   8.266122e+05        7.247865e+05   
n_empleados                     -1.496175e+04       -5.113620e+04   
utilidad_ejercicio               1.627621e+07        1.657677e+07   
utilidad_neta                    5.537558e+05        3.410551e+05   
liquidez_corriente              -1.662377e+05        0.000000e+00   
prueba_acida                     2.045903e+05        3.428472e+04   
end_activo                       3.115598e+04        4.094345e+04   
end_patrimonial                  7.625241e+05        3.462532e+04   
end_activo_fijo                 -3.112379e+04        0.000000e+00   
end_corto_plazo                 -1.463793e+05       -0.000000e+00   
end_largo_plazo                 -2.115259e+05       -5.536458e+04   
cobertura_interes                3.257216e+04        2.998723e+04   
apalancamiento                  -7.259819e+05        0.000000e+00   
apalancamiento_financiero        4.295715e+03        3.775793e+03   
end_patrimonial_ct               1.074038e+04        0.000000e+00   
end_patrimonial_nct              1.027924e+04        8.286964e+03   
apalancamiento_c_l_plazo        -1.104353e+04        0.000000e+00   
rot_cartera                      3.852633e+04        3.535403e+04   
rot_activo_fijo                  3.555713e+04        3.796517e+03   
rot_ventas                       8.715087e+04        1.770239e+04   
per_med_cobranza                -5.926692e+04       -4.475707e+04   
per_med_pago                     1.218606e+04        1.020888e+04   
impac_gasto_a_v                 -8.985391e+04       -8.637006e+04   
impac_carga_finan               -9.011229e+04       -7.519061e+04   
rent_neta_activo                 1.478064e+05        2.467433e+04   
margen_bruto                     1.088417e+05        1.109550e+05   
margen_operacional              -1.099608e+05       -1.289968e+05   
rent_neta_ventas                -2.163148e+05       -1.982777e+05   
rent_ope_patrimonio              4.363291e+04        3.750471e+04   
rent_ope_activo                  6.595121e+04        0.000000e+00   
roe                             -1.234671e+04       -6.188311e+03   
roa                             -9.794687e+04        2.172179e+04   
fortaleza_patrimonial           -2.431407e+04       -2.485074e+04   
gastos_financieros               5.616605e+05        5.628629e+05   
gastos_admin_ventas              5.802309e+06        5.928150e+06   
depreciaciones                   2.374406e+05        2.363894e+05   
amortizaciones                   2.976258e+04        4.784724e+04   
costos_ventas_prod               7.694830e+07        7.707636e+07   
deuda_total                     -6.106698e+04       -6.390710e+04   
deuda_total_c_plazo             -8.330810e+04       -8.887085e+04   
total_gastos                     2.177987e+07        2.170856e+07   

                           OLS Coefficients  
activos                        2.902318e+03  
patrimonio                     3.576716e+05  
utilidad_an_imp                1.764568e+05  
impuesto_renta                 7.274666e+05  
n_empleados                   -3.918085e+04  
utilidad_ejercicio             1.650916e+07  
utilidad_neta                  4.254553e+05  
liquidez_corriente            -4.471520e+05  
prueba_acida                   4.828738e+05  
end_activo                     1.970124e+04  
end_patrimonial                4.564141e+06  
end_activo_fijo               -1.650782e+04  
end_corto_plazo               -1.862862e+05  
end_largo_plazo               -2.337406e+05  
cobertura_interes              3.170820e+04  
apalancamiento                -4.533174e+06  
apalancamiento_financiero      4.177730e+03  
end_patrimonial_ct             1.651109e+08  
end_patrimonial_nct            2.460911e+07  
apalancamiento_c_l_plazo      -1.681961e+08  
rot_cartera                    3.656268e+04  
rot_activo_fijo                2.105640e+04  
rot_ventas                     9.473231e+04  
per_med_cobranza              -4.941610e+04  
per_med_pago                   1.211136e+04  
impac_gasto_a_v               -9.120222e+04  
impac_carga_finan             -8.122452e+04  
rent_neta_activo               2.201239e+05  
margen_bruto                   1.038475e+05  
margen_operacional            -1.154371e+05  
rent_neta_ventas              -2.087112e+05  
rent_ope_patrimonio            3.273467e+04  
rent_ope_activo                7.871489e+04  
roe                            5.823295e+03  
roa                           -1.855876e+05  
fortaleza_patrimonial         -2.366382e+04  
gastos_financieros             5.068169e+05  
gastos_admin_ventas            5.077702e+06  
depreciaciones                 2.346218e+05  
amortizaciones                 4.565041e+04  
costos_ventas_prod             7.705625e+07  
deuda_total                   -6.540293e+04  
deuda_total_c_plazo           -1.022102e+05  
total_gastos                   2.259806e+07  
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Graficar los resultados</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicciones Ridge&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lasso</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicciones Lasso&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_test</span><span class="p">)],</span> <span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_test</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Línea de referencia&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valores reales&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Valores predichos&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparación de Predicciones: Ridge vs Lasso&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/301ec823a8478cd57c41855eea504060cf6187245b2afcd061f45f2e2643fb3c.png" src="_images/301ec823a8478cd57c41855eea504060cf6187245b2afcd061f45f2e2643fb3c.png" />
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ProbModels.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Modelos de Probabilidad</p>
      </div>
    </a>
    <a class="right-next"
       href="Arboles.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Árboles de decisión</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-variables">10.1. Selección de variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-las-mejores-variables">10.1.1. Selección de las mejores variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-adelante">10.1.2. Selección <em>stepwise</em> hacia adelante</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-stepwise-hacia-atras">10.1.3. Selección <em>stepwise</em> hacia atrás</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo">10.1.3.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizacion">10.2. Regularización</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge">10.2.1. Regresión <em>Ridge</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-lasso">10.2.2. Regresión <em>Lasso</em></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">10.2.2.1. Ejemplo</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Víctor Morales Oñate
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>